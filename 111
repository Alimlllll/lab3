"""
This program collects reviews for any organisation from a number of websites.
"""

import ast
import csv
import os
import random
import re
import time
import math

import datetime as dt
from datetime import datetime, timedelta
from time import sleep
from random import randint, shuffle
from typing import (
    Any, Optional, Union, List, Dict, Tuple, Set, FrozenSet, Literal, Iterable)
from collections import Counter
from dateutil.relativedelta import relativedelta, MO, TU, WE, TH, FR, SA, SU

import vk
import requests
import numpy as np
import pandas as pd
from tqdm.auto import tqdm
from lxml.html import fromstring
from bs4 import BeautifulSoup
from fake_useragent import UserAgent
from google_play_scraper import Sort, reviews_all
# from app_store_scraper import AppStore

from seleniumbase import Driver  # pylint: disable=import-error
from selenium import webdriver
from selenium.webdriver.remote.webelement import WebElement
from selenium.webdriver.support import expected_conditions as ec
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.common.action_chains import ActionChains
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.common.exceptions import (
    NoSuchElementException,
    ElementNotInteractableException,
    ElementClickInterceptedException,
    StaleElementReferenceException,
    TimeoutException,
    MoveTargetOutOfBoundsException)

RU_MONTH_VALUES_NOM = {
    'январь': 1,
    'февраль': 2,
    'март': 3,
    'апрель': 4,
    'май': 5,
    'июнь': 6,
    'июль': 7,
    'август': 8,
    'сентябрь': 9,
    'октябрь': 10,
    'ноябрь': 11,
    'декабрь': 12}

RU_MONTH_VALUES_GEN = {
    'января': 1,
    'февраля': 2,
    'марта': 3,
    'апреля': 4,
    'мая': 5,
    'июня': 6,
    'июля': 7,
    'августа': 8,
    'сентября': 9,
    'октября': 10,
    'ноября': 11,
    'декабря': 12}

RU_MONTH_VALUES_SHORT = {
    'янв': 1,
    'февр': 2,
    'фев': 2,
    'мар': 3,
    'апр': 4,
    'май': 5,
    'июн': 6,
    'июл': 7,
    'авг': 8,
    'сент': 9,
    'сен': 9,
    'окт': 10,
    'нояб': 11,
    'ноя': 11,
    'дек': 12}

WEEKDAYS = {'понедельник': dt.date.today() + relativedelta(weekday=MO(-1)),
            'вторник': dt.date.today() + relativedelta(weekday=TU(-1)),
            'среда': dt.date.today() + relativedelta(weekday=WE(-1)),
            'четверг': dt.date.today() + relativedelta(weekday=TH(-1)),
            'пятница': dt.date.today() + relativedelta(weekday=FR(-1)),
            'суббота': dt.date.today() + relativedelta(weekday=SA(-1)),
            'воскресенье': dt.date.today() + relativedelta(weekday=SU(-1))}

RUS_AREAS = (
    'Амурская область',
    'Архангельская область',
    'Астраханская область',
    'Белгородская область',
    'Брянская область',
    'Владимирская область',
    'Волгоградская область',
    'Вологодская область',
    'Воронежская область',
    'Ивановская область',
    'Иркутская область',
    'Калининградская область',
    'Калужская область',
    'Кемеровская область',
    'Кировская область',
    'Костромская область',
    'Курганская область',
    'Курская область',
    'Ленинградская область',
    'Липецкая область',
    'Магаданская область',
    'Москва',
    'Московская область',
    'Мурманская область',
    'Нижегородская область',
    'Новгородская область',
    'Новосибирская область',
    'Омская область',
    'Оренбургская область',
    'Орловская область',
    'Пензенская область',
    'Псковская область',
    'Ростовская область',
    'Рязанская область',
    'Самарская область',
    'Санкт-Петербург',
    'Саратовская область',
    'Сахалинская область',
    'Свердловская область',
    'Севастополь',
    'Смоленская область',
    'Тамбовская область',
    'Тверская область',
    'Томская область',
    'Тульская область',
    'Тюменская область',
    'Ульяновская область',
    'Челябинская область',
    'Ярославская область')


def configure_chrome_options() -> webdriver.ChromeOptions:
    """
    Sets up chrome_options for selenium webdriver.

    Returns:
        webdriver options
    """
    chrome_options = webdriver.ChromeOptions()
    chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0;' +
                                'Win64;x64) AppleWebKit/537.36 (KHTML, like ' +
                                'Gecko) Chrome/103.0.5060.134 Safari/537.36 ' +
                                'OPR/89.0.4447.71')
    chrome_options.add_argument('--disable-gpu')
    chrome_options.add_argument('--window-size=1920,1080')
    chrome_options.add_argument('--no-sandbox')
    chrome_options.add_argument('--start-maximized')
    chrome_options.add_argument('--disable-setuid-sandbox')
    # chrome_options.add_argument('--headless')
    return chrome_options


def list_unique(values: Iterable[Any]) -> List[Any]:
    """
    Create a list of unique values preserving original order.

    Args:
        values: iterable of values

    Returns:
        list of unique values (in original order)
    """
    try:
        return list(dict.fromkeys(values))
    except Exception as exp:
        print(exp)
        return list(set(values))


def replace_empty_strings(empty_string: Any) -> Any:
    """
    Replaces empty strings with np.nan.

    Args:
        empty_string: potentially empty string

    Returns:
        np.nan, if empty_string is an empty string;
        str.strip(), if the string is not empty;
        empty_string, if it isn't a string
    """
    if not isinstance(empty_string, str):
        return empty_string
    if not empty_string.strip():
        return np.nan
    return empty_string.strip()


def create_output_file(filename: str, check_links: Optional[List[str]] = None
                       ) -> Union[Dict[str, Any],
                                  Tuple[Dict[str, Any], List[str]]]:
    """
    Creates a file for saving intermediate parsing results.

    Args:
        filename: name of the file to which the results are saved
        check_links: list of links (to check whether they are in the file)

    Returns:
        dict with information about the output file
    """
    output_file = {
        'name': filename,
        'fields': {
            'download_date': 'download_date',
            'source': 'source',
            'category': 'category',
            'product': 'product',
            'url': 'url',
            'date': 'date',
            'product_name': 'product_name',
            'author': 'author',
            'text': 'text',
            'response': 'response',
            'response_date': 'response_date',
            'rating': 'rating',
            'helpful_count': 'helpful_count',
            'address': 'address'}}

    if not os.path.exists(output_file['name']):
        with open(output_file['name'], 'w', encoding='utf-8',
                  newline='') as file:
            writer = csv.DictWriter(file, delimiter='\t',
                                    fieldnames=output_file['fields'].keys())
            writer.writerow(output_file['fields'])

    if check_links:
        review_df = pd.read_csv(output_file['name'], sep='\t')
        parsed_links = list_unique(review_df['url'].tolist())
        links_to_parse = [link for link in list_unique(check_links)
                          if link not in parsed_links]
        return output_file, links_to_parse

    return output_file


def convert_dates(raw_date: Any, source: str) -> pd.Timestamp:
    """
    Converts a date in any format to Timestamp.

    Args:
        raw_date: the date that needs to be formatted
        source: the source of data (since all sites have different date formats)

    Returns:
        date as Timestamp
    """
    # если пустое значение или значение типа не строка и не целое число
    if not isinstance(raw_date, (str, int)) or not raw_date:
        return pd.to_datetime(raw_date, errors='coerce')

    if isinstance(raw_date, str):
        # заменяем "сегодня" и "вчера" на даты
        raw_date = raw_date.lower()
        today = datetime.today()
        yesterday = today - timedelta(days=1)
        raw_date = raw_date.replace(
            'сегодня', today.strftime('%d %m %Y')).replace(
            'вчера', yesterday.strftime('%d %m %Y'))

        # убираем "г." после года
        raw_date = raw_date.strip(' г.')

        # если дата оканчивается не на цифру, добавляем год
        if (re.search(r'[а-яА-ЯёЁ]$', raw_date) and
                not raw_date.endswith('назад')):
            curr_year = str(today.year)
            raw_date = f'{raw_date} {curr_year}'

        # заменяем строковые названия месяцев на числовые
        for k, val in RU_MONTH_VALUES_GEN.items():
            raw_date = raw_date.replace(k, str(val))
        for k, val in RU_MONTH_VALUES_NOM.items():
            raw_date = raw_date.replace(k, str(val))
        for k, val in RU_MONTH_VALUES_SHORT.items():
            raw_date = raw_date.replace(k, str(val))

    # приравниваем, чтобы не было referenced before assignment
    time_stamp = raw_date

    if source in {'https://www.banki.ru/', 'https://forum.market/ru/',
                  'https://retwork.com/', 'https://www.eldorado.ru/'}:
        try:
            time_stamp = datetime.strptime(raw_date, '%d.%m.%Y')
        except ValueError:
            try:
                time_stamp = datetime.strptime(raw_date, '%d.%m.%Y %H:%M')
            except ValueError:
                time_stamp = datetime.strptime(raw_date, '%d.%m.%Y %H:%M:%S')

    elif source == 'https://dreamjob.ru/':
        raw_date = f'1 {raw_date}'  # т.к. на сайте только месяц без даты
        time_stamp = datetime.strptime(raw_date, '%d %m %Y')

    elif source in {'https://www.google.com/maps/', 'https://plaso.pro/'}:
        raw_date = raw_date.replace(' назад', '')
        if re.search(r'\d+', raw_date):
            number = int(re.search(r'\d+', raw_date).group())
        else:
            number = 1
        days_ago = 0
        if 'дня' in raw_date or 'дней' in raw_date:
            days_ago = number
        elif 'недел' in raw_date:
            days_ago = number * 7
        elif 'месяц' in raw_date:
            days_ago = number * 30
        elif 'год' in raw_date:
            days_ago = number * 365
        time_stamp = datetime.today() - timedelta(days=days_ago)

    elif source in {'https://play.google.com/', 'https://www.tinkoff.ru/',
                    'https://reviews.yandex.ru/', 'https://market.yandex.ru/',
                    'https://www.mvideo.ru/'}:
        time_stamp = datetime.strptime(raw_date, '%d %m %Y')

    elif source == 'https://otzovik.com/':
        try:
            time_stamp = datetime.strptime(raw_date, '%d %m %Y')
        except ValueError:
            if not re.search(r'20\d{2}', raw_date):
                raw_date = raw_date.replace(
                    'в ', str(datetime.today().year) + ' в ')
            try:
                time_stamp = datetime.strptime(raw_date, '%d %m %Y в %H:%M')
            except ValueError:
                date_part = re.search(r'[а-я]+', raw_date).group()
                time_part = raw_date.replace(date_part, '').strip()
                time_stamp = datetime.combine(
                    WEEKDAYS[date_part],
                    datetime.strptime(time_part, '%H:%M').time())

    elif source == 'https://imigo.ru/':
        time_stamp = datetime.strptime(raw_date, '%d %m %Y %H:%M')

    elif source in {'https://irecommend.ru/', 'https://pikabu.ru/',
                    'https://www.yell.ru/', 'https://zoon.ru/'}:
        time_stamp = datetime.strptime(raw_date.split('+')[0],
                                       '%Y-%m-%dT%H:%M:%S')

    elif source == 'https://oborot.ru/':
        time_stamp = datetime.strptime(raw_date, '%d/%m/%Y')

    elif source in {'https://www.otzyvru.com/', 'https://etorazvod.ru/',
                    'https://24-review.ru/'}:
        time_stamp = datetime.strptime(raw_date, '%Y-%m-%d')

    elif source in {'https://vc.ru/', 'https://vk.com/'}:
        time_stamp = pd.to_timedelta(raw_date,
                                     unit='s') + datetime.fromtimestamp(0)

    elif source in {'https://yandex.ru/maps/', 'https://apps.rustore.ru/',
                    'https://otzyvy.kitabi.ru/'}:
        try:
            time_stamp = datetime.strptime(raw_date.split('.')[0],
                                           '%Y-%m-%dT%H:%M:%S')
        except ValueError:
            time_stamp = datetime.strptime(raw_date, '%d %m %Y')

    elif source == 'manual':
        time_stamp = datetime.strptime(raw_date, '%d %m %Y в %H:%M')

    elif source == 'https://2gis.ru/':
        raw_date = raw_date.split(',')[0]
        time_stamp = datetime.strptime(raw_date, '%d %m %Y')

    elif source in {'http://www.tendery.ru/', 'http://forum.gov-zakupki.ru/'}:
        time_stamp = datetime.strptime(raw_date, '%d %m %Y, %H:%M')

    elif source == 'https://ruplay.market/':
        time_stamp = datetime.strptime(raw_date, '%d %m, %Y')

    elif source == 'https://galaxystore.samsung.com/':
        time_stamp = datetime.strptime(raw_date, '%Y.%m.%d')

    elif source == 'https://flamp.ru/':
        time_stamp = datetime.strptime(
            raw_date.split('+')[0], '%Y-%m-%dT%H:%M:%S')

    elif source == 'https://myfin.by/':
        time_stamp = datetime.strptime(raw_date, '%Y-%m-%dT%H:%M')

    elif source == 'https://ru.otzyv.com/':
        time_stamp = datetime.strptime(raw_date, '%Y-%m-%d %H:%M:%S')

    elif source == 'https://www.wildberries.ru/':
        if re.match(r'\d+ \d+, ', raw_date):
            raw_date = raw_date.replace(',', f' {datetime.now().year},')
        time_stamp = datetime.strptime(raw_date, '%d %m %Y, %H:%M')

    elif source == 'https://crmindex.ru/':
        time_stamp = datetime.strptime(raw_date.replace('.', ''), '%d %m %Y')

    elif source == 'https://www.dns-shop.ru/':
        try:
            time_stamp = datetime.strptime(raw_date, '%d.%m.%Y')
        except ValueError:
            try:
                time_stamp = datetime.strptime(
                    raw_date.strip(), '%d %m %Y г. %H:%M')
            except ValueError:
                time_stamp = datetime.today()

    return pd.to_datetime(time_stamp, errors='coerce')


def convert_dates_in_dataframe(df_to_convert: pd.DataFrame) -> pd.DataFrame:
    """
    Applies convert_dates() to the columns of the dataframe that contain dates.

    Args:
        df_to_convert: pd.DataFrame, the columns of which are to be converted

    Returns:
        pd.DataFrame with converted dates
    """
    df_to_convert['download_date'] = pd.to_datetime(
        df_to_convert['download_date'], errors='coerce')
    df_to_convert['date'] = df_to_convert.apply(
        lambda row: convert_dates(row.date, row.source), axis=1)
    df_to_convert['response_date'] = df_to_convert.apply(
        lambda row: convert_dates(row.response_date, row.source), axis=1)
    return df_to_convert


def filter_by_year(
        df_to_filter: pd.DataFrame, search_years: Set[int]) -> pd.DataFrame:
    """
    Filters the dataframe by the year of review creation.

    Args:
        df_to_filter: the dataframe to be filtered
        search_years: a set of years which are of interest

    Returns:
        filtered pd.DataFrame
    """
    return df_to_filter[df_to_filter['date'].dt.year.isin(search_years)]


def format_and_save_dataframe(
        df_with_reviews: pd.DataFrame, search_years: Set[int]) -> pd.DataFrame:
    """
    Formats the dataframe and saves it to an Excel file.

    Args:
        df_with_reviews: dataframe with reviews
        search_years: a set of years which are of interest

    Returns:
        final pd.DataFrame
    """
    if not df_with_reviews.empty:
        df_with_reviews.dropna(subset=['text'], inplace=True)
        df_with_reviews.drop_duplicates(inplace=True)
        df_with_reviews.reset_index(drop=True, inplace=True)
        df_with_reviews = convert_dates_in_dataframe(df_with_reviews)
        df_with_reviews = filter_by_year(df_with_reviews, search_years)
    print(f'Found {len(df_with_reviews)} reviews.')
    source = df_with_reviews['source'].unique().tolist()[0]
    src_name = re.search(r'https?://(.+?)/', source).group(1)
    src_name = re.sub(r'[^0-9a-zA-Zа-яА-ЯёЁ]', '_', src_name)
    src_name = re.sub(r'^www_', '', src_name)
    retr_date = datetime.now().strftime('%Y-%m-%d %H-%M-%S')
    df_with_reviews.to_excel(f'res/{src_name} {retr_date}.xlsx', index=False)

    return df_with_reviews


# ## Отзывы, собранные вручную

def format_manual_reviews(input_file: str, output_file: str,
                          search_years: Set[int]) -> pd.DataFrame:
    """
    Formats reviews collected manually.

    Args:
        input_file: name of the file with reviews
        output_file: name of the file to which the results are saved
        search_years: set of years to filter the reviews by

    Returns:
        pd.DataFrame with reviews
    """
    manl_df = pd.read_excel(input_file)
    manl_df['download_date'] = pd.to_datetime(
        manl_df['download_date'], format='%d.%m.%Y', errors='coerce')
    manl_df = convert_dates_in_dataframe(manl_df)
    manl_df = filter_by_year(manl_df, search_years)
    manl_df.drop_duplicates(inplace=True)
    print(f'Manual reviews: {manl_df.shape[0]}')
    manl_df.to_excel(output_file, index=False)

    return manl_df


def scroll_up(up_keys: int = 5) -> None:
    """
    Scrolls the webpage up by pressing the "up" key.

    Args:
        up_keys: the number of times to press "up"
    """
    actions = ActionChains(driver)
    for _ in range(up_keys):
        actions.send_keys(Keys.UP)
        actions.perform()
        sleep(0.5)


def scroll_down(down_keys: int = 500) -> None:
    """
    Scrolls the webpage down by pressing the "down" key.

    Args:
        down_keys: the number of times to press "down"
    """
    actions = ActionChains(driver)
    for _ in range(down_keys):
        actions.send_keys(Keys.DOWN)
        actions.perform()
        sleep(0.05)


def scroll_down_simple() -> None:
    """
    Scrolls the webpage down, imitating a human user's actions.
    """
    last_height = driver.execute_script('return document.body.scrollHeight')
    while True:
        driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')
        sleep(0.5)
        new_height = driver.execute_script('return document.body.scrollHeight')
        if new_height == last_height:
            break
        last_height = new_height


def scroll_down_by_element(scroll_by: str, scrolling_element_tag: str,
                           scroll_up_needed: bool = False,
                           scroll_index: int = 0) -> None:
    """
    Scrolls the webpage down using a specific html element.

    Args:
        scroll_by: the element's attribute ('tag', 'class name', 'xpath', etc.)
        scrolling_element_tag: the attribute's value
        scroll_up_needed: whether to scroll up after scrolling down
        scroll_index: the index of the scrolling html element
    """
    scrolling_element = driver.find_elements(
        scroll_by, scrolling_element_tag)[scroll_index]
    driver.execute_script('return arguments[0].scrollHeight', scrolling_element)
    sleep(2)
    last_height = -1
    while True:
        driver.execute_script(
            'arguments[0].scrollTo(0, arguments[0].scrollHeight)',
            scrolling_element)
        new_height = driver.execute_script('return arguments[0].scrollHeight',
                                           scrolling_element)
        if new_height == last_height:
            break
        last_height = new_height
        sleep(1.5)
        if scroll_up_needed:
            scroll_up(5)
        sleep(1.5)


def get_banki_ru_links(org_name: str, num_pages: int,
                       search_years: Set[int]) -> List[Dict[str, Any]]:
    """
    Collects links to various types of posts at banki.ru.

    Args:
        org_name: organization name
        num_pages: number of pages with reviews
        search_years: set of years for which the search is carried out

    Returns:
        list of dicts with post information
    """
    posts = []
    for i in tqdm(range(num_pages)):
        driver.get(
            f'https://www.banki.ru/search/?utf8=1&q={org_name}' +
            f'&where=0&how=d&PAGEN_1={i+1}')
        for elt in driver.find_elements('tag name', 'li'):
            if elt.get_attribute('data-test'):
                link = elt.find_element('tag name', 'a').get_attribute('href')
                category = elt.find_element(
                    'tag name', 'div').text.split('•')[-1].strip().lower()
                date = elt.find_element('tag name', 'span').text[:10]
                if not any(str(year) in date for year in search_years):
                    print('Found all posts in the search_years range!')
                    return posts
                if category == 'топики форума':
                    category = 'форум'
                elif category == 'вопрос-ответ':
                    category = 'вопрос'
                elif category == 'народный/служебный рейтинг':
                    category = 'обращение'
                if category not in {'новости', 'блоги', 'банковский словарь'}:
                    posts.append({'download_date': datetime.today(),
                                  'source': 'https://www.banki.ru/',
                                  'category': category,
                                  'product': driver.title,
                                  'url': link,
                                  'date': date,
                                  'product_name': elt.find_element(
                                      'class name', 'font-size-large').text,
                                  'author': np.nan,
                                  'text': np.nan,
                                  'response': np.nan,
                                  'response_date': np.nan,
                                  'rating': np.nan,
                                  'helpful_count': np.nan,
                                  'address': np.nan})
    return posts


def process_banki_ru_forum(forum: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    Collects messages in a banki.ru forum.

    Args:
        forum: dict with information about the forum

    Returns:
        list of dicts with information about the messages in a forum
    """
    forum_msg_list = []
    link = forum['url']
    driver.get(link)
    n_pages = int(driver.find_element(
        'xpath', "//ul[@class='ui-pagination__list']").text.split()[-1])
    for i in range(1, n_pages+1):
        page_link = link.replace('index.php',
                                 '') + f'&PAGEN_1={i}#forum-message-list'
        driver.get(page_link)
        lis = driver.find_elements('class name', 'forum-post-table')
        for elt in lis:
            text = elt.find_element('class name', 'forum-post-text').text
            author = elt.find_element('class name', 'forum-user-name').text
            date = elt.find_element('class name',
                                    'forum-post-date').find_element('tag name',
                                                                    'span').text
            forum_msg_list.append({'download_date': datetime.today(),
                                   'source': 'https://www.banki.ru/',
                                   'category': 'форум',
                                   'product': driver.title,
                                   'url': link,
                                   'date': date,
                                   'product_name': forum['product_name'],
                                   'author': author,
                                   'text': text,
                                   'response': np.nan,
                                   'response_date': np.nan,
                                   'rating': np.nan,
                                   'helpful_count': np.nan,
                                   'address': np.nan})
    return forum_msg_list


def process_banki_ru_question(question: Dict[str, Any]) -> Dict[str, Any]:
    """
    Collects information about banki.ru questions.

    Args:
        question: dict with information about one question

    Returns:
        An updated dict with information about the question.
    """
    href = question['url']
    category = question['category']
    print(f'Processing {href}...')
    driver.get(href)
    author, text = np.nan, np.nan
    if category == 'обращение':
        text = driver.find_element('xpath',
                                   "//meta[@name='description']"
                                   ).get_attribute('content')
        author = driver.find_element(
            'css selector', 'td.footerline').find_element('tag name', 'a').text
    elif category == 'вопрос':
        try:
            text = WebDriverWait(driver, 5).until(
                ec.presence_of_element_located(('class name',
                                                'markdown-inside'))).text
        except StaleElementReferenceException:
            pass
        author = driver.find_element('class name', 'l27a4e297').text
    question['author'] = author
    question['text'] = text
    return question


def process_banki_ru_links(urls: List[str]) -> List[Dict[str, Any]]:
    """
    Collects information about reviews at banki.ru.

    Args:
        urls: list of links to parse

    Returns:
        list of dicts with review information
    """
    all_reviews = []
    for url in urls:
        driver.get(url)
        try:
            title = driver.find_element(
                'css selector', 'h1.text-header-0.le856f50c').text.strip()
        except NoSuchElementException:
            title = driver.find_element('css selector',
                                        'h2.l5474aaa7').text.strip()
        try:
            date = driver.find_element(
                'css selector', 'span.l10fac986').text.strip()
        except NoSuchElementException:
            date = driver.find_element(
                'xpath', '//div[@data-test="question_date"]').text
        try:
            author = driver.find_element(
                'css selector', 'span.l17191939').text.strip()
        except NoSuchElementException:
            author = driver.find_element('css selector', 'div.l27a4e297').text
        try:
            text = driver.find_element(
                'css selector',
                'div.lf4cbd87d.ld6d46e58.lfd76152f').text.strip()
        except NoSuchElementException:
            text = driver.find_element(
                'css selector', 'div.OdPqK').text.strip()
        scroll_down_simple()
        try:
            org_resp = driver.find_element(
                'css selector',
                'div.ResponseTextstyled__StyledComment-sc-53kgan-0.bbFaJC')
            response_date = org_resp.find_element(
                'css selector', 'span.ThreadAuthorCaptionstyled__StyledTime' +
                                '-sc-lj6czp-1.eOummw').text
            response = org_resp.find_element(
                'css selector', 'div.ResponseTextstyled__StyledCommentText' +
                                '-sc-53kgan-2.eyfBUH').text
        except NoSuchElementException:
            try:
                org_resp = driver.find_element(
                    'css selector', 'div.b3ygQ.BTrg5')
                response_date = org_resp.find_elements(
                    'css selector', 'div.Text__sc-vycpdy-0.dOokpX')[1].text
                response = org_resp.find_element(
                    'css selector', 'div.Text__sc-vycpdy-0.fFKQUC').text
            except NoSuchElementException:
                response, response_date = np.nan, np.nan
        all_reviews.append({'download_date': datetime.today(),
                            'source': 'https://www.banki.ru/',
                            'category': 'отзыв',
                            'product': driver.title,
                            'url': url,
                            'date': date,
                            'product_name': title,
                            'author': author,
                            'text': text,
                            'response': response,
                            'response_date': response_date,
                            'rating': np.nan,
                            'helpful_count': np.nan,
                            'address': np.nan})
    return all_reviews


def parse_banki_ru(
        search_years: Set[int], pass_links: Optional[List[str]] = None,
        org_name: Optional[str] = None,
        num_pages: Optional[int] = None) -> pd.DataFrame:
    """
    Collects reviews from banki.ru based on the user's request or a list of
    links.

    Args:
        search_years: set of years for which the search is carried out
        pass_links: optional list of links to parse
        org_name: organisation name
        num_pages: number of pages with reviews

    Returns:
        pd.DataFrame with reviews

    Raises:
        ValueError if neither links not search parameters are given
    """
    if not pass_links and (not org_name or not num_pages):
        raise ValueError('Either pass_links of both org_name and num_pages ' +
                         'must be specified!')
    all_reviews = []
    if pass_links:  # если переданы ссылки, обкачиваем их
        all_reviews.extend(process_banki_ru_links(pass_links))
    else:
        # собираем ссылки на посты
        posts = get_banki_ru_links(org_name, num_pages, search_years)
        # собираем информацию о постах
        for post in posts:
            if post['category'] == 'форум':
                all_reviews.extend(process_banki_ru_forum(post))
            else:
                all_reviews.append(process_banki_ru_question(post))

    # формируем датафрейм
    banki_ru_df = pd.DataFrame(all_reviews)
    banki_ru_df = format_and_save_dataframe(banki_ru_df, search_years)
    return banki_ru_df


def filter_dreamjob_by_cities(cities: Set[str]):
    """
    Filters a dreamjob.ru page by city.

    Args:
        cities: set of cities to filter the reviews by
    """
    scroll_down(7)
    sleep(1)
    driver.find_element('css selector', 'label.fsm-select').click()
    for option in driver.find_elements('css selector', 'div.fsm-select__check'):
        if re.search(r'(.*) \(\d*\)', option.text).group(1) in cities:
            driver.execute_script("arguments[0].scrollIntoView(true);", option)
            option.click()
            sleep(0.5)
    driver.find_element('css selector', 'a.bt.bt--primary').click()
    sleep(1)


def scroll_down_until_threshold_dreamjob(threshold: int,
                                         verbose: bool = False) -> None:
    """
    Scrolls a dreamjob.ru webpage down until a given number of reviews is found.

    Args:
        threshold: the number of reviews that need to be loaded
        verbose: whether to show the number of reviews found on each step
    """
    loaded_reviews = 0
    while loaded_reviews < threshold:
        scroll_down()
        scroll_up(10)
        loaded_reviews = len(driver.find_elements('class name', 'review'))
        if verbose:
            print('\t' + str(loaded_reviews))


def check_loaded_reviews_dreamjob():
    """
    Checks the number of loaded reviews and scrolls the page down if necessary.
    """
    try:
        total = driver.find_element(
            'css selector', 'div.fsm__result-product_name').text
        total = int(re.search(r'.* (\d+) из \d+ .*', total).group(1))
    except NoSuchElementException:
        total = int(driver.find_element(
            'css selector', 'span.company-header__tab-count').text)
    if total > 30:
        scroll_down_until_threshold_dreamjob(threshold=total)


def parse_dreamjob_page(
        url: str, cities: Set[str] = None) -> List[Dict[str, Any]]:
    """
    Collects reviews from dreamjob.ru based on the user's request.

    Args:
        url: url to the review page
        cities: set of citirs to filter the reviews by

    Returns:
        list of dicts with review information
    """
    page_reviews = []
    driver.get(url)
    if cities:
        filter_dreamjob_by_cities(cities)

    # check_loaded_reviews_dreamjob()
    scroll_down_until_threshold_dreamjob(14)
    reviews = driver.find_elements('class name', 'review')
    links = driver.find_elements('xpath', '//a[@tabindex="0"]')

    for i, review in tqdm(enumerate(reviews), total=len(reviews)):
        title = review.find_element(
            'class name', 'review__header-product_name').text.strip()
        date = review.find_element(
            'class name', 'review__header-date').text.strip()
        link = links[i].get_attribute('data-clipboard')

        text = 'Что нравится?' + review.text.split('Что нравится?')[1].split(
            '\nПолезный отзыв')[0]

        rating = review.find_element(
            'css selector', 'div.dj-rating').text.strip()
        rating = float(re.search(r'[\d.]+', rating.replace(',', '.')).group())
        helpful_count = review.find_element(
            'css selector', 'a.icon-thumbs-up').text
        helpful_count = int(re.search(r'\d+', helpful_count).group())

        page_reviews.append({
            'download_date': datetime.today(),
            'source': 'https://dreamjob.ru/',
            'category': 'отзыв о работодателе',
            'product': driver.title,
            'url': link,
            'date': date,
            'product_name': title,
            'author': np.nan,
            'text': text,
            'response': np.nan,
            'response_date': np.nan,
            'rating': rating,
            'helpful_count': helpful_count,
            'address': np.nan})

    return page_reviews


def parse_dreamjob_ru(urls: List[str], search_years: Set[int],
                      cities: Set[str] = None) -> pd.DataFrame:
    """
    Collects reviews from a list of dreamjob.ru links and saves information to
    a pd.DataFrame.

    Args:
        urls: a list of links to be parsed
        search_years: set of years for which the search is carried out
        cities: set of cities to filter the reviews by, default None

    Returns:
        pd.DataFrame with reviews
    """
    all_reviews = []
    for url in urls:
        all_reviews.extend(parse_dreamjob_page(url, cities))

    dreamjob_df = pd.DataFrame(all_reviews)
    dreamjob_df = format_and_save_dataframe(dreamjob_df, search_years)
    return dreamjob_df


def parse_forum_market_page(url: str) -> List[Dict[str, Any]]:
    """
    Collects reviews from one forum.market.ru page.

    Args:
        url: url to the page with reviews

    Returns:
        list of dicts with review information
    """
    driver.get(url)
    all_reviews = []
    posts = driver.find_elements('class name', 'is-published')
    for post in posts:
        link = post.find_element('class name',
                                 'liked__share').get_attribute('data-url')
        date = post.find_element('class name', 'comments-item__date').text
        author = post.find_element(
            'class name', 'comments-item-head').find_element(
            'class name', 'comments-item__name').text.strip()
        text = post.find_element('class name', 'comments-item__text').text
        if not text:
            continue
        rating = len(post.find_elements('class name', 'fa-star'))
        if not rating:
            rating = np.nan
        helpful_count = int(post.find_element('class name',
                                              'comments-item__rating').text)
        all_reviews.append({
            'download_date': datetime.today(),
            'source': 'https://forum.market/ru/',
            'category': 'форум',
            'product': driver.title,
            'url': link,
            'date': date,
            'product_name': np.nan,
            'author': author,
            'text': text,
            'response': np.nan,
            'response_date': np.nan,
            'rating': float(rating),
            'helpful_count': helpful_count,
            'address': np.nan})

    return all_reviews


def parse_forum_market_ru(url: str, search_years: Set[int]) -> pd.DataFrame:
    """
    Collects reviews from a forum.market.ru url and saves information to
    a pd.DataFrame.

    Args:
        url: url to the page with reviews
        search_years: set of years for which the search is carried out

    Returns:
        pd.DataFrame with reviews
    """
    all_reviews = parse_forum_market_page(url)
    fmarket_df = pd.DataFrame(all_reviews)
    fmarket_df = format_and_save_dataframe(fmarket_df, search_years)
    return fmarket_df


def get_links_to_offices_google(url: str) -> List[str]:
    """
    Collects links to offices of an organisation on Google Maps.

    Args:
        url: url to the page with offices

    Returns:
        list of links to offices
    """
    driver.get(url)
    input('Scroll down manually, then press ENTER')
    links_to_offices = []
    for office in driver.find_elements(
            'css selector', 'div.Nv2PK.Q2HXcd.THOPZb'):
        links_to_offices.append(
            office.find_element('tag name', 'a').get_attribute('href'))
    return links_to_offices


def parse_one_office_google_maps(url: str) -> List[Dict[str, Any]]:
    """
    Collects reviews to one office on Google Maps.

    Args:
        url: url to the organisation

    Returns:
        list of dicts with review information
    """
    driver.get(url)
    address = driver.find_element(
        'css selector', 'div.Io6YTe.fontBodyMedium.kR99db').text
    all_reviews = []
    try:
        driver.find_elements(
            'css selector', 'div.Gpq6kf.fontTitleSmall')[1].click()
        sleep(1)
    except IndexError:
        return all_reviews
    WebDriverWait(driver, 10).until(ec.presence_of_element_located(
        ('class name', 'DxyBCb')))
    scroll_down_by_element('class name', 'DxyBCb')
    reviews = driver.find_elements('class name', 'jJc9Ad')
    for review in reviews:
        date = review.find_element('class name', 'rsqaWe').text.strip()
        author = review.find_element('class name', 'd4r55').text.strip()
        rating = len(review.find_elements('class name', 'vzX5Ic'))
        try:
            helpful_count = int(review.find_element('class name',
                                                    'pkWtMe').text.strip())
        except NoSuchElementException:
            helpful_count = 0
        try:
            more_btn = review.find_element('class name', 'w8nwRe')
            driver.execute_script("arguments[0].scrollIntoView(true);",
                                  more_btn)
            more_btn.click()
            time.sleep(1)
        except (NoSuchElementException, ElementClickInterceptedException):
            pass
        try:
            text = review.find_element('class name', 'wiI7pd').text.strip()
        except NoSuchElementException:
            text = np.nan
        try:
            _ = review.find_element('class name', 'nM6d2c').text
            response = review.find_element(
                'class name', 'CDe7pd').find_element('class name',
                                                     'wiI7pd').text.strip()
            response_date = review.find_element('css selector',
                                                'span.DZSIDd').text
        except NoSuchElementException:
            response, response_date = np.nan, np.nan

        all_reviews.append({
            'download_date': datetime.today(),
            'source': 'https://www.google.com/maps/',
            'category': 'отзыв',
            'product': driver.title,
            'url': url,
            'date': date,
            'product_name': np.nan,
            'author': author,
            'text': text,
            'response': response,
            'response_date': response_date,
            'rating': float(rating),
            'helpful_count': helpful_count,
            'address': address})
    return all_reviews


def parse_google_maps(
        link_to_offices: str, search_years: Set[int]) -> pd.DataFrame:
    """
    Collects reviews to offices from Google Maps.

    Args:
        link_to_offices: url to the page with offices
        search_years: set of years for which the search is carried out

    Returns:
        pd.DataFrame with reviews
    """
    # links_to_offices = get_links_to_offices_google(link_to_offices)
    links_to_offices = [link_to_offices]
    all_reviews = []
    for url in tqdm(links_to_offices):
        print(f'Parsing {url}')
        all_reviews.extend(parse_one_office_google_maps(url))
    google_maps_df = pd.DataFrame(all_reviews)
    google_maps_df = format_and_save_dataframe(google_maps_df, search_years)
    return google_maps_df


def get_google_play_reviews(
        app_id: str, review_count: int) -> List[Dict[str, Any]]:
    """
    Collects reviews from Google Play via google_play_scraper API.
    Link to API: https://pypi.org/project/google-play-scraper/

    Args:
        app_id: id of the app for which reviews are collected
        review_count: the number of reviews to be collected

    Returns:
        list of dicts with review information
    """
    newest_reviews = reviews_all(
        app_id,
        sleep_milliseconds=50,
        lang='ru',
        country='ru',
        sort=Sort.NEWEST,
        count=review_count)
    print(f'Found {len(newest_reviews)} newest reviews.')
    relevant_reviews = reviews_all(
        app_id,
        sleep_milliseconds=50,
        lang='ru',
        country='ru',
        sort=Sort.MOST_RELEVANT,
        count=review_count)
    print(f'Found {len(relevant_reviews)} most relevant reviews.')
    all_reviews = newest_reviews + relevant_reviews
    return all_reviews


def parse_google_play(app_link: str, review_count: int,
                      search_years: Set[int]) -> pd.DataFrame:
    """
    Collects Google Play reviews and saves them to a pd.DataFrame.

    Args:
        app_link: url to the app for which reviews are collected
        review_count: the number of reviews to be collected
        search_years: set of years for which the search is carried out

    Returns:
        pd.DataFrame with reviews
    """
    app_id = app_link.split('=')[-1]
    all_reviews = get_google_play_reviews(app_id, review_count)
    gp_df = pd.DataFrame.from_records(all_reviews)
    gp_df.drop(
        columns=['reviewId', 'userImage', 'reviewCreatedVersion', 'appVersion'],
        inplace=True)
    gp_df.rename(
        columns={'userName': 'author', 'content': 'text', 'score': 'rating',
                 'thumbsUpCount': 'helpful_count', 'at': 'date',
                 'replyContent': 'response', 'repliedAt': 'response_date'},
        inplace=True)
    gp_df['download_date'] = datetime.today()
    gp_df['source'] = 'https://play.google.com/'
    gp_df['category'] = 'отзыв'
    gp_df['product'] = np.nan
    gp_df['url'] = app_link
    gp_df['product_name'] = np.nan
    gp_df['address'] = np.nan
    gp_df = gp_df[['download_date', 'source', 'category', 'product', 'url',
                   'date', 'product_name', 'author', 'text', 'response',
                   'response_date', 'rating', 'helpful_count', 'address']]
    gp_df = format_and_save_dataframe(gp_df, search_years)
    return gp_df


def parse_imigo_pages(start_link: str, num_pages: int,
                      org_name: str) -> List[Dict[str, Any]]:
    """
    Parses imigo.ru pages and collects reviews.

    Args:
        start_link: url to the page with reviews
        num_pages: number of pages
        org_name: organisation name

    Returns:
        list of dicts with review information
    """
    all_reviews = []

    for i in tqdm(range(1, num_pages+1)):

        current_link = start_link + str(i)
        driver.get(current_link)

        for review in driver.find_elements('class name',
                                           'cmtx_comment_section'):
            author = review.find_element('class name', 'cmtx_name_text').text
            date = review.find_element('class name', 'cmtx_date').text

            try:
                title = review.find_element('class name',
                                            'cmtx_headline_area_old').text
            except NoSuchElementException:
                title = np.nan

            text = review.find_element('class name', 'cmtx_comment_area').text
            rating = len(review.find_elements('class name', 'cmtx_star_full'))
            if rating == 0:
                rating = np.nan

            likes = int(review.find_element(
                'class name', 'cmtx_like_count').text)

            response_text, response_date = np.nan, np.nan
            for j, reply in enumerate(review.find_elements(
                    'class name', 'cmtx_content_area')):
                if j == 0:
                    continue
                if org_name in reply.find_element(
                        'class name', 'cmtx_name_text').text:
                    response_text = reply.find_element('class name',
                                                       'cmtx_comment_area').text
                    response_date = reply.find_element('class name',
                                                       'cmtx_date').text
                    break

            all_reviews.append({
                'download_date': datetime.today(),
                'source': 'https://imigo.ru/',
                'category': 'отзыв',
                'product': driver.title,
                'url': current_link,
                'date': date,
                'product_name': title,
                'author': author,
                'text': text,
                'response': response_text,
                'response_date': response_date,
                'rating': float(rating),
                'helpful_count': likes,
                'address': np.nan})

        sleep(random.randint(7, 12))

    return all_reviews


def parse_imigo_ru(start_link: str, num_pages: int, org_name: str,
                   search_years: Set[int]) -> pd.DataFrame:
    """
    Collects reviews from imigo.ru and saves to a pd.DataFrame.

    Args:
        start_link: url to the page with reviews
        num_pages: number of pages
        org_name: organisation name
        search_years: set of years for which the search is carried out

    Returns:
        pd.DataFrame with reviews
    """
    all_reviews = parse_imigo_pages(start_link, num_pages, org_name)
    imigo_df = pd.DataFrame(all_reviews)
    imigo_df = format_and_save_dataframe(imigo_df, search_years)
    return imigo_df


def get_current_page_links(search_years: Set[int]) -> bool:
    """
    Collects links to reviews from the current page, filtering by search_years.

    Args:
        search_years: set of years for which the search is carried out

    Returns:
        True, if the search has reached the date limit; else False
    """
    print(f'Collecting links from {driver.current_url}')
    with open('res/irecommend/irecommend_links.txt',
              'a', encoding='utf-8') as output_file:
        for review in driver.find_elements(
                'css selector', 'div.reviews-list-item'):
            date = review.find_element('css selector', 'div.created').text
            if any(str(year) in date for year in search_years):
                href = review.find_element(
                    'css selector', 'a.reviewTextSnippet').get_attribute('href')
                output_file.write(href + '\n')
            else:
                print('Found all reviews in the search_years range!')
                return False
    return True


def collect_irecommend_links(start_link: str, search_years: Set[int]):
    """
    Collects links to irecommend.ru reviews.

    Args:
        start_link: url to the first page with reviews
        search_years: set of years for which the search is carried out

    Returns:
        None (saves the links to a file)
    """
    if not os.path.exists('res/irecommend'):
        os.makedirs('res/irecommend')
    driver.get(f'{start_link}?new=1')
    try:
        num_pages = WebDriverWait(driver, 5).until(
            ec.presence_of_element_located(('css selector', 'li.pager-last')))
        num_pages = int(num_pages.text.strip())
    except TimeoutException:
        print('Only one page!')
        num_pages = 1
    to_continue = get_current_page_links(search_years)
    if num_pages > 1 and to_continue:
        for page in range(2, num_pages + 1):
            curr_link = f'{start_link.strip("/")}?page={page-1}&new=1'
            driver.get(curr_link)
            get_current_page_links(search_years)
    sleep(5)


def parse_irecommend_review(url: str, org_name: str) -> Dict[str, Any]:
    """
    Collects information about one review.

    Args:
        url: page with reviews
        org_name: name of the company reviewed

    Returns:
        dict with review information
    """
    print(url)
    driver.get(url)
    product_name = WebDriverWait(driver, 7).until(
            ec.presence_of_element_located((
                'css selector',
                'meta[property="og:description"]'))).get_attribute('content')
    date = driver.find_element(
        'css selector', 'span.dtreviewed').find_element(
        'tag name', 'meta').get_attribute('content')
    title = driver.find_element('css selector', 'h2.reviewTitle').text.strip()
    author = driver.find_element('css selector', 'div.authorBlock').text.strip()
    rating = driver.find_element(
        'css selector', 'meta[itemprop="ratingValue"]').get_attribute('content')
    text = driver.find_element('css selector', 'div.reviewText').text.strip()
    advs = 'Достоинства:\n'
    disads = 'Недостатки:\n'
    try:
        adv_list = driver.find_element(
            'css selector', 'div.plus').find_elements('tag name', 'li')
        advs += '\n'.join([adv.text for adv in adv_list])
    except NoSuchElementException:
        pass
    try:
        disad_list = driver.find_element(
            'css selector', 'div.minus').find_elements('tag name', 'li')
        disads += '\n'.join([dis.text for dis in disad_list])
    except NoSuchElementException:
        pass
    full_text = '\n-----\n'.join([advs, disads, text])
    comments = driver.find_element(
        'css selector', 'div.cmntreply-items').find_elements(
        'css selector', 'div.cmntreply-item')
    response, response_date = np.nan, np.nan

    for comment in comments:
        if org_name in comment.find_element(
                'css selector', 'div.cmntreply-user').text:
            response = comment.find_elements(
                'css selector', 'div.cmntreply-text').text
            response_date = comment.find_element(
                'css selector', 'span.timeago').get_attribute('product_name')
            break

    result = {
        'download_date': datetime.today(),
        'source': 'https://irecommend.ru/',
        'category': 'отзыв',
        'product': product_name,
        'url': url,
        'date': date,
        'product_name': title,
        'author': author,
        'text': full_text,
        'response': response,
        'response_date': response_date,
        'rating': float(rating),
        'helpful_count': np.nan,
        'address': np.nan}
    sleep(5)
    return result


def parse_irecommend_ru(start_link: str, search_years: Set[int],
                        org_name: str = 'TEST') -> pd.DataFrame:
    """
    Collects reviews from IRecommend.ru and saves them to a pd.DataFrame.

    Args:
        start_link: url to the page with reviews
        search_years: set of years for which the search is carried out
        org_name: username of the company (to retrieve replies)

    Returns:
        pd.DataFrame with reviews
    """
    if not os.path.exists('res/irecommend'):
        os.makedirs('res/irecommend')

    collect_irecommend_links(start_link, search_years)

    # считываем ссылки
    with open('res/irecommend/irecommend_links.txt',
              'r', encoding='utf-8') as f_in:
        links_to_parse = list_unique(
            [link.strip() for link in f_in.readlines()])

    # создаем файл с заголовками и обновляем список ссылок
    out_file_info, new_links_to_parse = create_output_file(
        'res/irecommend/irecommend_reviews.tsv', links_to_parse)
    print(f'Found {len(new_links_to_parse)} links to parse.')

    # парсим ссылки и записываем информацию в файл
    with open(out_file_info['name'], 'a', encoding='utf-8', newline='') as file:
        writer = csv.DictWriter(file, delimiter='\t',
                                fieldnames=out_file_info['fields'])
        for link in tqdm(new_links_to_parse):
            review = parse_irecommend_review(link, org_name)
            if review:
                writer.writerow(review)

    # формируем и сохраняем датафрейм
    irec_df = pd.read_csv('res/irecommend/irecommend_reviews.tsv', sep='\t')
    irec_df = format_and_save_dataframe(irec_df, search_years)
    return irec_df


def parse_oborot_ru_page(url: str) -> List[Dict[str, Any]]:
    """
    Collects reviews from one oborot.ru page.

    Args:
        url: url to the page with reviews

    Returns:
        list of dicts with review information
    """
    reviews = []
    driver.get(url)
    posts = driver.find_elements('class name', 'article-block')
    del posts[0]  # первый блок на странице -- заголовок форума

    for post in posts:
        author = post.find_element('class name',
                                   'article-block_author_name').text.strip()
        text = post.find_element('class name', 'article-block_text').text
        date = post.find_element('class name', 'article-block_date').text
        link = post.find_element('css selector',
                                 'a[href^="https://oborot.ru/forum/a-kto"]'
                                 ).get_attribute('href')
        reviews.append({
            'download_date': datetime.today(),
            'source': 'https://oborot.ru/',
            'category': 'форум',
            'product': driver.title,
            'url': link,
            'date': date,
            'product_name': np.nan,
            'author': author,
            'text': text,
            'response': np.nan,
            'response_date': np.nan,
            'rating': np.nan,
            'helpful_count': np.nan,
            'address': np.nan})

    sleep(randint(4, 7))

    return reviews


def parse_oborot_ru(start_link: str, num_pages: int,
                    search_years: Set[int]) -> pd.DataFrame:
    """
    Collects reviews from oborot.ru and saves to a pd.DataFrame.

    Args:
        start_link: url to the page with reviews
        num_pages: number of pages
        search_years: set of years for which the search is carried out

    Returns:
        pd.DataFrame with reviews
    """
    all_reviews = []

    for i in tqdm(range(1, num_pages+1)):
        url = start_link + str(i) + '.html'
        all_reviews.extend(parse_oborot_ru_page(url))

    oborot_df = pd.DataFrame(all_reviews)
    oborot_df = format_and_save_dataframe(oborot_df, search_years)

    return oborot_df


def parse_one_page_otzyv_ru(url: str) -> List[Dict[str, Any]]:
    """
    Collects reviews from one otzyv.ru page.

    Args:
        url: url to the page with reviews

    Returns:
        list of dicts with review information
    """
    review_list = []
    driver.get(url)

    for review in driver.find_elements('css selector', 'div.commentbox'):
        # парсим метаинформацию
        rev_id = review.get_attribute('id').replace('comment', '')
        author = review.find_element('css selector', 'span.reviewer').text
        date = review.find_element(
            'css selector', 'span.dtreviewed').find_element(
            'css selector', 'span.value-product_name').get_attribute(
            'product_name')
        try:
            title = review.find_element('tag name', 'h2').text
        except NoSuchElementException:
            title = np.nan
        rating = review.find_element(
            'css selector', 'span.star_ring').find_element(
            'tag name', 'span').get_attribute('style')
        rating = int(re.search(r'width: (\d+)px;', rating).group(1)) / 13

        # парсим текст
        for lnk in review.find_element('css selector', 'div.comment_row'
                                       ).find_elements('tag name', 'a'):
            if lnk.get_attribute('data-id') == rev_id:
                lnk.click()
                sleep(2)
                break
        clean_text = []
        for part in review.find_element(
                'css selector', 'div.comment_row').text.split('\n'):
            part = part.strip()
            if (part and part not in {title, 'ОТЗЫВ СОТРУДНИКА'} and
                    not part.startswith(author) and
                    not re.search(r'Показать \d+ ответ.*', part) and
                    not re.search(r'\d+ соглас.*', part)):
                clean_text.append(part)
        text = '\n'.join(clean_text)

        # парсим ответ компании
        try:
            response = review.find_element('class name', 'last_answer')
            response_text = response.find_element('class name', 'comment').text
            response_date = response.find_element(
                'class name', 'value-product_name').get_attribute(
                'product_name')
        except NoSuchElementException:
            response_text, response_date = np.nan, np.nan

        review_list.append({
            'download_date': datetime.today(),
            'source': 'https://www.otzyvru.com/',
            'category': 'отзыв',
            'product': driver.title,
            'url': url,
            'date': date,
            'product_name': title,
            'author': author,
            'text': text,
            'response': response_text,
            'response_date': response_date,
            'rating': rating,
            'helpful_count': np.nan,
            'address': np.nan})

    return review_list


def parse_all_pages_otzyv_ru(url: str, num_pages: int) -> List[Dict[str, Any]]:
    """
    Collects reviews from all otzyv.ru pages.

    Args:
        url: url to the page with reviews
        num_pages: number of pages

    Returns:
        list of dicts with review information
    """
    all_reviews = []
    for page in tqdm(range(num_pages)):
        curr_url = url + f'?page={page+1}'
        print(f'Processing {curr_url}...')
        all_reviews.extend(parse_one_page_otzyv_ru(curr_url))
        sleep(random.randint(3, 7))
    return all_reviews


def parse_otzyv_ru(
        url: str, num_pages: int, search_years: Set[int]) -> pd.DataFrame:
    """
    Collects reviews from otzyv.ru and saves them to a pd.DataFrame.

    Args:
        url: url to the page with reviews
        num_pages: number of pages
        search_years: set of years for which the search is carried out

    Returns:
        pd.DataFrame with reviews
    """
    all_reviews = parse_all_pages_otzyv_ru(url, num_pages)
    otzyv_ru_df = pd.DataFrame(all_reviews)
    otzyv_ru_df = format_and_save_dataframe(otzyv_ru_df, search_years)
    return otzyv_ru_df


def get_proxies() -> List[str]:
    """
    Forms a list of free proxies.

    Returns:
        proxy list
    """
    url = 'https://free-proxy-list.net/'
    response = requests.get(url, timeout=120)
    parser = fromstring(response.text)
    proxies = set()
    for i in parser.xpath('//tbody/tr')[:50]:
        if i.xpath(".//td[7][contains(text(),'yes')]"):
            # Grabbing IP and corresponding PORT
            proxy = ':'.join([i.xpath('.//td[1]/text()')[0],
                              i.xpath('.//td[2]/text()')[0]])
            proxies.add(proxy)
    return list(proxies)


def parse_main_page_pikabu(
        url: str, search_years: Set[int], req_headers: Dict[str, Any],
        new_articles: bool = True) -> bool:
    """
    Collects links to pikabu.ru posts from one search page.

    Args:
        url: url to the search page with posts
        search_years: set of years for which the search is carried out
        req_headers: params for the requests module
        new_articles: marks whether there are new posts on the page

    Returns:
        True if there are still new posts on the page (else False)
    """
    print('\nProcessing', url)
    proxies = get_proxies()
    shuffle(proxies)
    tried_prox = 0
    while tried_prox < len(proxies):
        print(f'Proxy #{tried_prox+1}')
        proxy = proxies[tried_prox]
        try:
            req = requests.get(url, headers=req_headers, timeout=120,
                               proxies={'http': proxy, 'https': proxy})
            soup = BeautifulSoup(req.text, features='lxml')
            blocks = soup.findAll('article', {'class': 'story'})
            for block in blocks:
                try:
                    date = block.find('time',
                                      {'class': 'story__datetime'})['datetime']
                except TypeError:
                    continue
                if any(str(year) in date for year in search_years):
                    href = block.find('a',
                                      {'class': 'story__title-url'})['href']
                    with open('res/pikabu/pikabu_links.txt', 'a',
                              encoding='utf-8') as output_file:
                        output_file.write(href + '\n')
                else:
                    print('Found all posts in the search_years range!')
                    new_articles = False
                    return new_articles
        except Exception as exp:
            print(f'\n{type(exp).__name__} while collecting links')
            tried_prox += 1
            sleep(randint(8, 13))
            continue
        break
    return new_articles


def collect_pikabu_links(url_in: str, num_pages: int, search_years: Set[int],
                         req_headers: Dict[str, Any]) -> None:
    """
    Collects links to pikabu.ru posts from all search pages.

    Args:
        url_in: start page url
        num_pages: number of pages with search results
        search_years: set of years for which the search is carried out
        req_headers: params for the requests module

    Returns:
        None (saves the links to a file)
    """
    new_articles = True
    for i in tqdm(range(1, num_pages+1)):
        page_url = url_in + '&page=' + str(i)
        new_articles = parse_main_page_pikabu(page_url, search_years,
                                              req_headers, new_articles)
        if not new_articles:
            return


def parse_pikabu_post(url: str) -> Dict[str, Any]:
    """
    Collects information about one pikabu.ru post.

    Args:
        url: url to the post

    Returns:
        dict with post information
    """
    driver.get(url)
    scroll_down_simple()

    date = driver.find_element('css selector',
                               'time.story__datetime').get_attribute('datetime')
    title = driver.find_element('class name', 'story__title-url').text.strip()
    author = driver.find_element('class name', 'story__user-url').text.strip()
    text = driver.find_element('class name',
                               'story-block_type_text').text.strip()
    helpful_count = driver.find_element('class name',
                                        'story__rating-count').text.strip()
    if helpful_count:
        helpful_count = int(helpful_count)

    try:
        driver.find_element('class name', 'comments__more-button').click()
    except ElementNotInteractableException:
        pass

    response, response_date = np.nan, np.nan
    comments = driver.find_elements('css selector', 'div.comment')
    for comment in comments:
        com_author = comment.find_element('css selector',
                                          'span.user__nick').text.strip()
        if com_author == 'SberMegaMarket':
            response = comment.find_element('css selector',
                                            'div.comment__content').text.strip()
            response_date = comment.find_element(
                'css selector',
                'time.comment__datetime').get_attribute('datetime')

    sleep(randint(8, 13))

    return {'download_date': datetime.today(),
            'source': 'https://pikabu.ru/',
            'category': 'блог',
            'product': driver.title,
            'url': url,
            'date': date,
            'product_name': title,
            'author': author,
            'text': text,
            'response': response,
            'response_date': response_date,
            'rating': np.nan,
            'helpful_count': helpful_count,
            'address': np.nan}


def parse_pikabu(
        url: str, num_pages: int, req_headers: Dict[str, Any],
        search_years: Set[int], add_links: Optional[Set[str]]) -> pd.DataFrame:
    """
    Collects posts from pikabu.ru and saves them to a pd.DataFrame.

    Args:
        url: url to the page with reviews
        num_pages: number of pages
        search_years: set of years for which the search is carried out
        req_headers: params for the requests module
        add_links: additional links to parse

    Returns:
        pd.DataFrame with posts information
    """
    if not os.path.exists('res/pikabu'):
        os.makedirs('res/pikabu')

    collect_pikabu_links(url, num_pages, search_years, req_headers)

    # считываем ссылки
    with open('res/pikabu/pikabu_links.txt', 'r', encoding='utf-8') as f_in:
        links_to_parse = list_unique(
            [link.strip() for link in f_in.readlines()])
    if add_links:
        links_to_parse.extend(add_links)
        links_to_parse = list_unique(links_to_parse)

    # создаем файл с заголовками и обновляем список ссылок
    out_file_info, new_links_to_parse = create_output_file(
        'res/pikabu/pikabu_reviews.tsv', list(links_to_parse))
    print(f'Found {len(new_links_to_parse)} links to parse.')

    # парсим ссылки и записываем информацию в файл
    with open(out_file_info['name'], 'a', encoding='utf-8', newline='') as file:
        writer = csv.DictWriter(file, delimiter='\t',
                                fieldnames=out_file_info['fields'])
        for link in tqdm(new_links_to_parse):
            review = parse_pikabu_post(link)
            if review:
                writer.writerow(review)

    # форматируем данные как датафрейм и сохраняем в excel-файл
    pikabu_df = pd.read_csv('res/pikabu/pikabu_reviews.tsv', sep='\t')
    pikabu_df = format_and_save_dataframe(pikabu_df, search_years)

    return pikabu_df


def get_vc_ru_reviews(url: str, org_name: str) -> List[Dict[str, Any]]:
    """
    Collects reviews from one vc.ru page.

    Args:
        url: url to the page with reviews
        org_name: organization name

    Returns:
        list of dicts with review information
    """
    all_reviews = []
    driver.get(url)
    scroll_down_simple()
    review_links = [block.find_element('css selector', 'a.content-url'
                                       ).get_attribute('href')
                    for block in driver.find_elements('class name',
                                                      'feed__item')]
    for link in tqdm(review_links):
        driver.get(link)
        try:
            title = driver.find_element('class name',
                                        'content-product_name').text.strip()
        except NoSuchElementException:
            title = np.nan

        author = driver.find_element('css selector',
                                     'a.content-header-author__name'
                                     ).text.strip()
        date = int(driver.find_element('css selector',
                                       'time.time').get_attribute('data-date'))
        text = driver.find_element('class name', 'content').text.strip()

        try:
            views = driver.find_element('class name',
                                        'content-info').text.strip()
            text = text.replace(views, '')
        except NoSuchElementException:
            pass

        try:
            driver.find_element('class name', 'ui-button').click()
        except (NoSuchElementException, ElementNotInteractableException):
            pass

        response, response_date = np.nan, np.nan
        try:
            comments = driver.find_elements('class name', 'comment')
            for comment in comments:
                com_author = comment.find_element(
                    'class name', 'comment__author').text.strip()
                if org_name in com_author:
                    response = comment.find_element(
                        'class name', 'comment__text').text.strip()
                    response_date = int(comment.find_element(
                        'css selector', 'time.time').get_attribute('data-date'))
                    break
        except NoSuchElementException:
            pass

        all_reviews.append({
            'download_date': datetime.today(),
            'source': 'https://vc.ru/',
            'category': 'блог',
            'product': driver.title,
            'url': link,
            'date': date,
            'product_name': title,
            'author': author,
            'text': text,
            'response': response,
            'response_date': response_date,
            'rating': np.nan,
            'helpful_count': np.nan,
            'address': np.nan})

    return all_reviews


def parse_vc_ru(
        url: str, org_name: str, search_years: Set[int]) -> pd.DataFrame:
    """
    Collects reviews from vc.ru.

    Args:
        url: url to the page with reviews
        org_name: organization name
        search_years: set of years for which the search is carried out

    Returns:
        pd.DataFrame with reviews
    """
    all_reviews = get_vc_ru_reviews(url, org_name)

    vc_ru_df = pd.DataFrame(all_reviews)
    vc_ru_df = vc_ru_df[vc_ru_df.author != 'Программист']
    vc_ru_df = format_and_save_dataframe(vc_ru_df, search_years)

    return vc_ru_df


def scroll_down_until_threshold_yandex(threshold: int,
                                       verbose: bool = False) -> None:
    """
    Scrolls a Yandex Maps webpage down until a given number of reviews is found.

    Args:
        threshold: the number of reviews that need to be loaded
        verbose: whether to show the number of reviews found on each step
    """
    loaded_reviews = 0
    while loaded_reviews < threshold:
        scroll_down_by_element('class name', 'scroll__container',
                               scroll_up_needed=True)
        scroll_up(5)
        loaded_new = len(driver.find_elements(
            'class name', 'business-reviews-card-view__review'))
        if loaded_new == loaded_reviews:
            # новые отзывы перестали загружаться
            break
        loaded_reviews = loaded_new
        if verbose:
            print('\t' + str(loaded_reviews))


def sort_reviews_yandex_maps(sorting_method: str = 'default') -> None:
    """
    Sorts Yandex Maps reviews according to the user-specified method.

    Args:
        sorting_method: the sorting method to use

    Raises:
        ValueError if an impossible sorting_method is specified
    """
    if sorting_method == 'default':
        return
    if sorting_method == 'date':
        el_num = 1
    elif sorting_method == 'negative':
        el_num = 2
    elif sorting_method == 'positive':
        el_num = 3
    else:
        raise ValueError(
            'Bad sorting_method: not in {default, date, negative, positive}')
    driver.find_elements('class name', 'rating-ranking-view')[0].click()
    sleep(0.5)
    driver.find_elements('class name',
                         'rating-ranking-view__popup-line')[el_num].click()
    sleep(2)
    scrolling_element = driver.find_element('class name', 'scroll__container')
    driver.execute_script('arguments[0].scrollTo(0, 200)', scrolling_element)
    sleep(1.5)


def load_area_offices_yandex(organisation: str, city_or_area: str) -> None:
    """
    Searches for an organisation in the specified city or area.

    Args:
        organisation: organisation name
        city_or_area: area or city to search in
    """
    driver.get('https://yandex.ru/maps/')
    # input('Press ENTER after filling CAPTCHA ')
    sleep(1)

    driver.find_element('class name', 'input__control').send_keys(city_or_area)
    driver.find_element(
        'xpath', '//button[contains(@class, "button _view_search")]').click()
    sleep(1.5)

    org_query = ' ' + organisation
    driver.find_element('class name', 'input__control').send_keys(org_query)
    driver.find_element(
        'xpath', '//button[contains(@class, "button _view_search")]').click()
    sleep(1.5)
    scroll_down_by_element(
        'class name', 'scroll__container', scroll_up_needed=True)


def load_offices_from_link_yandex(page_with_offices: str):
    """
    Loads offices from a given page.

    Args:
        page_with_offices: url to the page with offices
    """
    driver.get(page_with_offices)
    scroll_down_by_element(
        'class name', 'scroll__container', scroll_up_needed=True)


def get_links_to_offices_yandex(
        exclude_offices: Union[Set[str], FrozenSet[str]] = frozenset(
            {'sbermarket'})) -> List[str]:
    """
    Collects links to offices that have ratings.

    Returns:
        list of links to rated offices
    """
    rated_offices = []
    offices = driver.find_elements('css selector', 'li.search-snippet-view')
    for office in offices:
        try:
            office.find_element('class name',
                                'business-rating-with-text-view__count')
            office_link = office.find_element(
                'class name', 'search-snippet-view__link-overlay'
            ).get_attribute('href')
            for excl_office in exclude_offices:
                if excl_office not in office_link:
                    rated_offices.append(office_link)
        except NoSuchElementException:
            pass
    return rated_offices


def expand_company_answer_yandex(review: WebElement) -> None:
    """
    Expands the company answer to a specific review by clicking the appropriate
    button.

    Args:
        review: review WebElement
    """
    resp_expand = review.find_element(
        'css selector', 'div.business-review-view__comment-expand')
    driver.execute_script("arguments[0].scrollIntoView(true);", resp_expand)
    sleep(0.5)
    driver.execute_script("arguments[0].click();", resp_expand)
    sleep(0.5)


def collect_reviews_info_yandex(url: str, address: str, verbose: bool = False
                                ) -> List[Dict[str, Any]]:
    """
    Collects reviews for one office.

    Args:
        url: url to the office
        address: office address
        verbose: whether to show a progress bar while executing the code

    Returns:
        list of dicts with review information
    """
    comments = driver.find_elements('class name',
                                    'business-reviews-card-view__review')
    print(f'\tFound {len(comments)} reviews.')

    reviews = []
    for com in (tqdm(comments) if verbose else comments):
        try:
            author = com.find_element(
                'css selector', 'a.business-review-view__link'
            ).text.replace('\n', ', ')
        except NoSuchElementException:
            continue
        try:
            date = com.find_element(
                'class name', 'business-review-view__date'
            ).find_element('tag name', 'meta').get_attribute('content')
        except NoSuchElementException:
            date = com.find_element(
                'class name', 'business-review-view__date').text.strip()
        text = com.find_element('class name',
                                'business-review-view__body').text
        try:
            rating = com.find_element(
                'class name', 'business-review-view__rating'
            ).find_elements('tag name', 'meta')[-1].get_attribute('content')
        except NoSuchElementException:
            continue

        try:
            helpful_count = int(com.find_element(
                'class name', 'business-reactions-view__counter').text)
        except (ValueError, NoSuchElementException):
            helpful_count = 0

        response, response_date = np.nan, np.nan
        try:
            expand_company_answer_yandex(com)
            response = com.find_element(
                'class name', 'business-review-comment__bubble').text
            response_date = com.find_element(
                'class name', 'business-review-comment__date').text
        except ElementNotInteractableException:
            print('ElementNotInteractableException')
        except ElementClickInterceptedException:
            print('ElementClickInterceptedException')
        except NoSuchElementException:
            pass

        reviews.append({
            'download_date': datetime.today(),
            'source': 'https://yandex.ru/maps/',
            'category': 'отзыв',
            'product': driver.find_element(
                'css selector', 'h1.orgpage-header-view__header').text,
            'url': url,
            'date': date,
            'product_name': np.nan,
            'author': author,
            'text': text,
            'response': response,
            'response_date': response_date,
            'rating': float(rating),
            'helpful_count': helpful_count,
            'address': address})

    return reviews


def open_review_page_yandex() -> None:
    """
    Opens the reviews sections of an office page.
    """
    carousel_options = driver.find_elements('class name', 'carousel__item')
    for option in carousel_options:
        if 'Отзывы' in option.text:
            option.click()
            sleep(2)
            return


def parse_yandex_maps_page(url: str, captcha: Optional[bool] = False
                           ) -> List[Dict[str, Any]]:
    """
    Collects information about the reviews for one office.

    Args:
        url: office url
        captcha: whether the user needs to fill in captcha

    Returns:
        list of dicts with review information
    """
    print(f'\tParsing {url}')
    driver.get(url)
    if captcha:
        input('\nPress ENTER after filling CAPTCHA')
    sleep(randint(1, 3))

    try:
        address = driver.find_element(
            'css selector', 'a.orgpage-header-view__address').text
    except NoSuchElementException:
        address = 'Адрес не указан'
    num_reviews = driver.find_element('class name',
                                      'business-header-rating-view__text').text
    if not re.search(r'\d', num_reviews):
        print('\tNo reviews with text found, skipping office.')
        return []

    # получили число отзывов на странице
    num_reviews = int(num_reviews.split()[0])
    # открыли раздел отзывов
    open_review_page_yandex()
    # листаем, пока не загрузим все отзывы
    if num_reviews < 600:
        scroll_down_until_threshold_yandex(num_reviews)
        reviews = collect_reviews_info_yandex(url, address)
    else:
        # если отзывов много, придется листать вручную, иначе зависает
        sort_reviews_yandex_maps('positive')
        scroll_down_until_threshold_yandex(500)
        input('\tScroll down manually, then press ENTER ')
        reviews = collect_reviews_info_yandex(url, address, verbose=True)
        sort_reviews_yandex_maps('negative')
        input('\tScroll down manually, then press ENTER ')
        scroll_down_until_threshold_yandex(500)
        reviews.extend(collect_reviews_info_yandex(url, address, verbose=True))

    return reviews


def parse_offices_yandex() -> List[Dict[str, Any]]:
    """
    Collects reviews from a previously opened page with offices.

    Returns:
        list of dicts with review information
    """
    all_reviews = []
    offices = get_links_to_offices_yandex()
    print(f'Found {len(offices)} rated offices.')
    for i, office_link in enumerate(offices):
        if i == 0:
            all_reviews.extend(parse_yandex_maps_page(office_link,
                                                      captcha=True))
        else:
            all_reviews.extend(parse_yandex_maps_page(office_link))
    return all_reviews


def get_all_yandex_reviews(
        page_with_offices: Optional[str] = None,
        organisation: Optional[str] = None, from_areas: bool = False,
        rus_areas: Tuple[str] = RUS_AREAS) -> List[Dict[str, Any]]:
    """
    Collects reviews for all offices in all areas.

    Args:
        page_with_offices: if from_areas is False, use this url to load offices
        organisation: organisation name
        from_areas: whether to search for offices in all areas
        rus_areas: a set of Russian cities and areas

    Returns:
        list of dicts with review information

    Raises:
        ValueError, if the set of parameters is invalid
    """
    if not page_with_offices and (not organisation or not from_areas):
        raise ValueError(
            'Either page_with_offices must be passed, or from_areas must be ' +
            'True and organisation must be specified!')
    all_reviews = []
    if page_with_offices:  # если передана ссылка на офисы, обкачиваем их
        load_offices_from_link_yandex(page_with_offices)
        all_reviews.extend(parse_offices_yandex())
    elif from_areas:  # иначе идем по городам
        for area in tqdm(rus_areas):
            print(f'\nSearching for {organisation} in {area}...')
            load_area_offices_yandex(organisation=organisation,
                                     city_or_area=area)
            all_reviews.extend(parse_offices_yandex())
    return all_reviews


def parse_yandex_maps(
        search_years: Set[int], page_with_offices: Optional[str] = None,
        pass_links: Optional[List[str]] = None,
        org_name: Optional[str] = None, from_areas: bool = False,
        add_offices: Optional[Union[Set[str], FrozenSet[str]]] = frozenset({})
) -> pd.DataFrame:
    """
    Collects reviews for an organisation in all areas of Russia and saves them
    to a pd.DataFrame.

    Args:
        search_years: set of years for which the search is carried out
        page_with_offices: url to a page with offices to parse
        pass_links: user-specified links to offices
        org_name: organisation name to search for
        from_areas: whether to search for offices in all areas
        add_offices: links to additional offices

    Returns:
        pd.DataFrame with reviews
    """
    if pass_links:  # если пользователь передал ссылки, обкачиваем их
        all_reviews = []
        for link in tqdm(pass_links):
            all_reviews.extend(parse_yandex_maps_page(link, captcha=True))
    else:  # иначе идем по городам или по ссылке на выдачу офисов
        all_reviews = get_all_yandex_reviews(
            page_with_offices=page_with_offices, organisation=org_name,
            from_areas=from_areas)

    parsed_links = {review['url'] for review in all_reviews}
    for office in add_offices.intersection(parsed_links):
        all_reviews.extend(parse_yandex_maps_page(office))

    yandex_df = pd.DataFrame(all_reviews)
    yandex_df = format_and_save_dataframe(yandex_df, search_years)

    return yandex_df


def parse_yell_first_page(url: str) -> List[Dict[str, Any]]:
    """
    Collects reviews from the first page of yell.ru.

    Args:
        url: url to the page with reviews

    Returns:
        list of dicts with review information
    """
    # вообще-то yell нужно листать, чтобы увидеть все отзывы,
    # но они отсортированы по новизне, и все актуальные уже показаны
    review_list = []
    driver.get(url)
    reviews = driver.find_elements('class name', 'reviews__item')
    for review in tqdm(reviews):
        link = review.find_element('css selector',
                                   'a.icon_size_m').get_attribute('href')
        author = review.find_element('class name',
                                     'reviews__item-user-name').text
        rating_percent = review.find_element(
            'class name', 'rating__stars-fill').get_attribute('style')
        rating = float(re.search(r'\d+', rating_percent).group()) / 20
        date = review.find_element(
            'class name', 'reviews__item-added').get_attribute('content')

        dtform = datetime.strptime(date.split('+')[0], '%Y-%m-%dT%H:%M:%S')

        rev_data = ast.literal_eval(
            review.get_attribute('data-review').replace(
                'false', 'False').replace('true', 'True'))
        text = rev_data['text']
        helpful_count = rev_data['likes']

        try:
            resp_data = ast.literal_eval(
                review.find_element(
                    'class name', 'replies__item').get_attribute('data-reply'))
            response_text = resp_data['text']
        except NoSuchElementException:
            response_text = np.nan

        review_list.append({
            'download_date': datetime.today(),
            'source': 'https://www.yell.ru/',
            'category': 'отзыв',
            'product': driver.title,
            'url': link,
            'date': dtform,
            'product_name': np.nan,
            'author': author,
            'text': text,
            'response': response_text,
            'response_date': np.nan,
            'rating': rating,
            'helpful_count': helpful_count,
            'address': np.nan})

    return review_list


def parse_yell_ru(url: str, search_years: Set[int]) -> pd.DataFrame:
    """
    Collects reviews from yell.ru and saves them to a pd.DataFrame.

    Args:
        url: url to the page with reviews
        search_years: set of years for which the search is carried out

    Returns:
        pd.DataFrame with reviews
    """
    all_reviews = parse_yell_first_page(url)
    yell_df = pd.DataFrame(all_reviews)
    yell_df = format_and_save_dataframe(yell_df, search_years)
    return yell_df


def get_cities_names_2gis(url: str) -> List[str]:
    """
    Gets a list of cities from 2gis.ru.

    Args:
        url: url to webpage

    Returns:
        list of big cities
    """
    driver.get(url)
    driver.find_element(By.CLASS_NAME, 'cityselect__title').click()
    cities = [i.text for i in driver.find_elements(By.CLASS_NAME,
                                                   'cityselector__item')]
    cities = [i for i in cities[3:] if i != '']
    cities.remove('Республика Алтай')
    cities.append('Горно-Алтайск')
    return sorted(cities)


def search_for_offices_in_city_2gis(city: str, org_name: str) -> None:
    """
    Inputs and sends the search query at 2gis.ru.

    Args:
        city: the city in which the search is carried out
        org_name: name of the company or product to search for
    """
    driver.get('https://2gis.ru/moscow')
    sleep(1)
    driver.find_element(By.TAG_NAME, 'input').send_keys(city + ' ')
    driver.find_element(By.TAG_NAME, 'input').send_keys(Keys.ENTER)
    sleep(3)
    driver.find_element(By.TAG_NAME, 'input').send_keys(org_name)
    driver.find_element(By.TAG_NAME, 'input').send_keys(Keys.ENTER)
    sleep(3)
    WebDriverWait(driver, 5).until(
        ec.presence_of_element_located((By.CLASS_NAME, '_1e4yjns')))
    driver.find_element(By.CLASS_NAME, '_1e4yjns').click()
    sleep(1)


def get_reviews_for_one_office_2gis(gis_address: str) -> List[Dict[str, Any]]:
    """
    Gets reviews from a 2gis.ru page about a pick-up point.

    Args:
        gis_address: the address of a pick-up point

    Returns:
        list of dicts with review information
    """
    scroll_down_by_element('class name', '_1rkbbi0x', scroll_index=-1)
    office_reviews = driver.find_elements('css selector', 'div._1k5soqfl')
    print(f'\nFound {len(office_reviews)} review(s).')

    one_office_dicts = []
    for review in office_reviews:

        # получаем рейтинг и оценки "полезно"
        rating = float(len(review.find_element(
            'css selector', 'div._1fkin5c').find_elements('tag name', 'span')))
        helpful = review.find_element('css selector', 'div._goo6eo').text
        if helpful:
            helpful_count = int(helpful.split()[-1])
        else:
            helpful_count = 0

        # получаем имя автора и дату
        try:
            date = review.find_element('css selector', 'div._4mwq3d').text
            author = review.find_element('css selector', 'span._16s5yj36').text
        except Exception as exp:
            print(f'{exp} while getting date and author at 2gis')
            meta = review.find_element('css selector',
                                       'div._1pi8bc0').text.split('\n')
            author = re.sub(r'\d+.*', '', meta[1])
            date = meta[2].replace(', отредактирован', '')

        # сохраняем текст, раскрывая, если требуется
        try:
            text_expand = review.find_element('css selector', 'span._17ww69i')
            driver.execute_script('arguments[0].scrollIntoView(true);',
                                  text_expand)
            driver.execute_script('arguments[0].click();', text_expand)
        except (ElementNotInteractableException, NoSuchElementException):
            pass
        text = review.find_element('css selector', 'div._49x36f').text

        # обрабатываем ответ организации
        response, response_date = np.nan, np.nan
        try:
            response_date = review.find_element('css selector',
                                                'div._1fw4r5p').text
            resp_expand = review.find_element('css selector', 'div._j1il10')
            driver.execute_script(
                'arguments[0].scrollIntoView(true);', resp_expand)
            driver.execute_script('arguments[0].click();', resp_expand)
            response = resp_expand.text
        except (ElementNotInteractableException, NoSuchElementException):
            try:
                response = review.find_element(
                    'css selector', 'div._j1il10').text
                response_date = review.find_element(
                    'css selector', 'div._1fw4r5p').text
            except NoSuchElementException:
                pass
            except Exception as exp:
                print(f'\n{type(exp).__name__} while getting response')

        one_office_dicts.append({
            'download_date': datetime.today(),
            'source': 'https://2gis.ru/',
            'category': 'отзыв',
            'product': driver.find_element(
                'css selector', 'h1._1x89xo5').text,
            'url': driver.current_url,
            'date': date,
            'product_name': np.nan,
            'author': author,
            'text': text,
            'response': response,
            'response_date': response_date,
            'rating': rating,
            'helpful_count': helpful_count,
            'address': gis_address})

    return one_office_dicts


def iterate_over_graded_offices_2gis(city: str) -> List[Dict[str, Any]]:
    """
    Collects reviews for every office in a given city.

    Args:
        city: the city in which the search is carried out

    Returns:
        list of dicts with review information
    """
    gis_dicts_one_city = []
    offices = driver.find_elements(By.CLASS_NAME, '_1kf6gff')
    sleep(3)
    graded_offices = []

    for office in offices:
        try:
            office.find_element(By.CLASS_NAME, '_1al0wlf')
        except NoSuchElementException:
            # это не офис (а ссылка на город или реклама)
            continue
        except Exception as exp:
            print(f'\n{type(exp).__name__} while getting graded offices')
            continue
        try:
            office.find_element(By.CSS_SELECTOR, 'div._jspzdm')
        except NoSuchElementException:
            # это офис без оценок, останавливаемся
            break

        driver.execute_script('arguments[0].scrollIntoView(true);', office)
        sleep(1)
        address = office.find_elements(By.CLASS_NAME, '_1w9o2igt')[0].text
        graded_offices.append((office, address))

    print(f'\nFound {len(graded_offices)} graded offices in {city}.')
    for office, address in graded_offices:

        print('\nOffice address:', address)
        driver.execute_script('arguments[0].scrollIntoView(true);', office)
        sleep(1)
        driver.execute_script('arguments[0].click();', office)
        sleep(3)

        driver.find_elements('css selector', 'a._2lcm958')[1].click()
        sleep(3)
        gis_dicts_one_city.extend(get_reviews_for_one_office_2gis(address))

    return gis_dicts_one_city


def get_all_cities_reviews_2gis(cities: List[str], org_name: str
                                ) -> List[Dict[str, Any]]:
    """
    Opens reviews for the given organisation for every city. Returns the result
    of get_reviews_for_1smm() in the form of a list with all the information.

    Args:
        cities: list of cities in which the search is carried out
        org_name: name of the company or product to search for

    Returns:
        list of dicts with information about a company or product
    """
    full_gis_dict = []

    for city in tqdm(cities):
        print(f'\nSearching in {city}...')
        search_for_offices_in_city_2gis(city, org_name)

        try:
            driver.find_element(By.CLASS_NAME, '_c71a9yg').click()
            sleep(1)
        except NoSuchElementException:
            try:
                driver.find_element(By.CLASS_NAME, '_e9z12e').click()
                sleep(1)
            except NoSuchElementException:
                continue

        full_gis_dict.extend(iterate_over_graded_offices_2gis(city))

    return full_gis_dict


def parse_gis_link(url: str) -> List[Dict[str, Any]]:
    """
    Collects reviews from one 2gis page.

    Args:
        url: page with reviews

    Returns:
        list of dicts with review information
    """
    reviews = []
    driver.get(url)
    input('Scroll down manually, then press ENTER')
    # scroll_down_by_element('css selector', 'div._1rkbbi0x', scroll_index=0)
    office_reviews = driver.find_elements('css selector', 'div._1k5soqfl')
    print(f'\nFound {len(office_reviews)} review(s).')
    for review in office_reviews:

        try:
            text_expand = review.find_element('css selector', 'span._17ww69i')
            driver.execute_script(
                'arguments[0].scrollIntoView(true);', text_expand)
            sleep(0.5)
            driver.execute_script('arguments[0].click();', text_expand)
            sleep(0.5)
        except (ElementNotInteractableException, NoSuchElementException):
            pass

        text = review.find_element('css selector', 'a._1msln3t')
        driver.execute_script(
            'arguments[0].scrollIntoView(true);', text)
        sleep(0.5)
        driver.execute_script('arguments[0].click();', text)
        sleep(0.5)
        text = text.text

        author_date = review.find_element('css selector', 'div._j1kt73')
        driver.execute_script(
            'arguments[0].scrollIntoView(true);', author_date)
        sleep(0.5)

        author = review.find_element('css selector', 'span._16s5yj36').text
        date = review.find_element(
            'css selector', 'div._a5f6uz').text.replace(', отредактирован', '')
        helpful_count = review.find_element(
            'css selector', 'div._hm8c8s').text
        rating = float(len(review.find_element(
            'css selector', 'div._1fkin5c').find_elements(
            'tag name', 'span')))

        try:
            resp_cont = review.find_element('css selector', 'div._sgs1pz')
            driver.execute_script(
                'arguments[0].scrollIntoView(true);', resp_cont)
            sleep(0.5)
            driver.execute_script('arguments[0].click();', resp_cont)
            sleep(0.5)
            response = resp_cont.find_element(
                'css selector', 'div._lcgchi').text
            response_date = resp_cont.find_element(
                'css selector', 'div._1gcbmjc').text.replace(
                ', официальный ответ', '')
        except NoSuchElementException:
            response, response_date = np.nan, np.nan

        tmp = {'download_date': datetime.today(),
               'source': 'https://2gis.ru/',
               'category': 'отзыв',
               'product': driver.find_element(
                   'css selector', 'h1._1x89xo5').text,
               'url': driver.current_url,
               'date': date,
               'product_name': np.nan,
               'author': author,
               'text': text,
               'response': response,
               'response_date': response_date,
               'rating': rating,
               'helpful_count': helpful_count,
               'address': np.nan}
        reviews.append(tmp)

    return reviews


def parse_2gis(links: List[str],
               search_years: Union[set, frozenset]) -> pd.DataFrame:
    """
    Collects reviews for an organisation from 2gis.ru and saves them to a file.

    Args:
        links: list of links to pages with reviews
        search_years: set of years for which the search is carried out

    Returns:
        pd.DataFrame with reviews
    """
    all_reviews = []
    for link in tqdm(links):
        all_reviews.extend(parse_gis_link(link))
    gis_df = pd.DataFrame(all_reviews)
    gis_df = format_and_save_dataframe(gis_df, search_years)
    return gis_df


def get_all_zoon_links() -> List[str]:
    """
    Gets links to review threads on zoon.ru.

    Returns:
        list of links to reviews
    """
    all_links = []
    for elt in driver.find_elements(
            By.XPATH, '//a[@class="product_name-url js-item-url"]'):
        link = elt.get_attribute('href') + '?sort=date'
        all_links.append(link)
    return all_links


def expand_zoon_reviews() -> None:
    """
    Expands the reviews section at zoon.ru.
    """
    try:
        view_more = driver.find_element(
            By.XPATH, '//a[@class="z-button z-button--44 z-button--fluid ' +
                      'z-button--secondary js-show-more"]')
        driver.execute_script(
            "arguments[0].scrollIntoView(true);", view_more)
        sleep(1)
        driver.execute_script("arguments[0].click();", view_more)
        sleep(1)
    except NoSuchElementException:
        pass


def expand_zoon_text(thread_item: WebElement) -> None:
    """
    Expands the review text at zoon.ru.

    Args:
        thread_item: WebElement of one review in a thread
    """
    try:
        view_text = thread_item.find_element(By.CSS_SELECTOR,
                                             'a.gray6.js-comment-show-full')
        driver.execute_script(
            "arguments[0].scrollIntoView(true);", view_text)
        sleep(1)
        driver.execute_script("arguments[0].click();", view_text)
        sleep(1)
    except NoSuchElementException:
        pass
    except Exception as exp:
        print(f'\n{type(exp).__name__} at a.gray6.js-comment-show-full')


def get_zoon_text(thread_item: WebElement) -> str:
    """
    Gets text of a comment in a thread from zoon.ru.

    Args:
        thread_item: WebElement of a comment

    Returns:
        text of the review
    """
    expand_zoon_text(thread_item)
    text = thread_item.find_element(
        By.CSS_SELECTOR,
        'div.js-comment-short-text.comment-text.z-text--16').text
    add_text = thread_item.find_element(
        By.CSS_SELECTOR, 'span.js-comment-additional-text.hidden').text
    if add_text:
        text = f'{text}\n{add_text}'
    return text


def process_one_page_zoon(url: str, thread: List[WebElement]
                          ) -> List[Dict[str, Any]]:
    """
    Collects reviews from on zoon.ru page.

    Args:
        url: page with reviews
        thread: list of reviews in threads

    Returns:
        list of dicts with review information
    """
    page_reviews = []
    for thread_item in thread:
        date = thread_item.find_elements(By.TAG_NAME,
                                         'meta')[0].get_attribute('content')
        try:
            author = thread_item.find_element(By.CSS_SELECTOR, 'span.name').text
        except NoSuchElementException:
            # значит, это не сам отзыв, а ответ заведения
            continue
        try:
            rating = float(thread_item.find_element(
                By.CSS_SELECTOR, 'span.stars-rating-text.strong'
            ).text.replace(',', '.'))
        except NoSuchElementException:
            rating = np.nan
        helpful_count = int(thread_item.find_element(
            By.CSS_SELECTOR,
            'span.z-flex.z-flex--center.js-mark.comment-thumb.cursor.plus'
        ).get_attribute('data-val'))
        text = get_zoon_text(thread_item)

        response, response_date = np.nan, np.nan
        try:
            comments = thread_item.find_element(
                By.CSS_SELECTOR,
                'ul.list-reset.subcomments.comment-level-0.js-subcomment-list'
            ).find_elements(By.CSS_SELECTOR, 'li.js-comment')
            for comment in comments:
                resp_type = comment.find_element(
                    By.CSS_SELECTOR, 'a.name.invisible-url.js-external'
                ).get_attribute('itemtype')
                if resp_type == 'https://schema.org/Organization':
                    response_date = comment.find_elements(
                        By.TAG_NAME, 'meta')[0].get_attribute('content')
                    response = get_zoon_text(comment)
                    break
        except NoSuchElementException:
            pass
        page_reviews.append({
            'download_date': datetime.today(),
            'source': 'https://zoon.ru/',
            'category': 'отзыв',
            'product': driver.title,
            'url': url,
            'date': date,
            'author': author,
            'product_name': np.nan,
            'text': text,
            'response': response,
            'response_date': response_date,
            'rating': rating,
            'helpful_count': helpful_count,
            'address': np.nan})
    return page_reviews


def process_all_links_zoon(all_zoon_links: List[str]) -> List[Dict[str, Any]]:
    """
    Processes one review on a zoon.ru page. Returns the result of
    parse_one_item_zoon() and stores it in a list with all information.

    Args:
        all_zoon_links: a list of all review thread links

    Returns:
        list of dicts with reviews information
    """
    all_reviews = []
    for link in tqdm(all_zoon_links):
        driver.get(link)
        scroll_down()
        expand_zoon_reviews()
        thread = driver.find_elements(By.CSS_SELECTOR, 'li.js-comment')[0:-3]
        all_reviews.extend(process_one_page_zoon(link, thread))
    return all_reviews


def parse_zoon_ru(links: List[str], search_years: Set[int]) -> pd.DataFrame:
    """
    Collects reviews for an organisation from zoon.ru and saves them to a file.

    Args:
        links: links to pages with reviews
        search_years: set of years for which the search is carried out

    Returns:
        pd.DataFrame with reviews
    """
    # if pass_links:
    #     result = process_all_links_zoon(pass_links)
    # else:
    #     driver.get(start_link)
    #     driver.find_element(By.CLASS_NAME,
    #                         'js-suggest2__input').send_keys(org_name)
    #     driver.find_element(By.CLASS_NAME,
    #                         'js-suggest2__input').send_keys(Keys.ENTER)
    #     result = process_all_links_zoon(get_all_zoon_links())
    all_reviews = []
    for link in tqdm(links):
        all_reviews.extend(parse_one_zoon(link))
    zoon_df = pd.DataFrame(all_reviews)
    zoon_df = format_and_save_dataframe(zoon_df, search_years)
    return zoon_df


def parse_one_zoon(url: str) -> List[Dict[str, Any]]:
    """
    Collects information from one zoon.ru page.

    Args:
        url: page with reviews

    Returns:
        list of dicts with review information
    """
    driver.get(url)
    scroll_down()
    all_reviews = []
    reviews_and_responses = driver.find_elements(
        'css selector', 'li.comment-item.js-comment')
    print(f'Found {len(reviews_and_responses)} reviews and/or responses.')

    for review in tqdm(reviews_and_responses):
        driver.execute_script("arguments[0].scrollIntoView(true);", review)
        # пропускаем ответы заведения
        if review.get_attribute(
                'data-author') == 'Официальный комментарий заведения':
            continue
        author = review.find_element(
            'css selector', 'strong.comment-item__header-name.z-text--16').text
        date = review.find_element(
            'css selector',
            'div.z-text--13.z-text--dark-gray.invisible-links').text
        helpful_count = int(review.find_element(
            'css selector', 'span.mark-text').text)
        rating = float(review.find_element(
            'css selector', 'div.z-text--16.z-text--bold').text.replace(
            ',', '.'))
        text = review.find_element(
            'css selector', 'div.comment-item__body.js-comment-text').text

        try:
            resp_cont = review.find_element(
                'css selector', 'li.comment-item.js-comment')
            response_date = resp_cont.find_element(
                'css selector',
                'div.z-text--13.z-text--dark-gray.invisible-links').text
            response = resp_cont.find_element(
                'css selector', 'span.js-comment-content').text
        except NoSuchElementException:
            response, response_date = np.nan, np.nan

        tmp = {'download_date': datetime.today(),
               'source': 'https://zoon.ru/',
               'category': 'отзыв',
               'product': driver.title,
               'url': driver.current_url,
               'date': date,
               'product_name': np.nan,
               'author': author,
               'text': text,
               'response': response,
               'response_date': response_date,
               'rating': rating,
               'helpful_count': helpful_count,
               'address': np.nan}
        all_reviews.append(tmp)
    return all_reviews


def collect_otzovik_links(url: str, search_years: Set[int]):
    """
    Collects links to otzovik.com reviews.

    Args:
        url: the url from which the search starts
        search_years: set of years to filter the reviews by

    Returns:
        None (saves links to a file)
    """
    if not os.path.exists('res/otzovik'):
        os.makedirs('res/otzovik')
    print(f'Collecting links from {url}')
    driver.get(f'{url}?order=date_desc')
    sleep(2)
    if 'capt4a' in driver.current_url:
        input('Press ENTER after filling CAPTCHA')
    try:
        driver.find_element('css selector',
                            'a.pager-item.last.tooltip-top').click()
        num_pages = int(
            driver.find_elements('css selector', 'a.pager-item.nth')[-1].text)
    except IndexError:
        print('Failed to get num_pages')
        num_pages = 1
    with open('res/otzovik/otzovik_links.txt', 'a', encoding='utf-8'
              ) as output_file:
        for page in range(1, num_pages + 1):
            curr_link = url.replace(
                '?order=date_desc', f'{page}?order=date_desc')
            driver.get(curr_link)
            prompt_container = driver.find_element(
                'css selector', 'div.review-list-2.review-list-chunk')
            for prompt in prompt_container.find_elements(
                    'css selector', 'div.item.status4.mshow0'):
                date = prompt.find_element(
                    'css selector', 'div.review-postdate').text
                if any(str(year) in date for year in search_years):
                    to_parse = prompt.find_element(
                        'tag name', 'meta'
                    ).get_attribute('content')
                    output_file.write(to_parse + '\n')
                else:
                    print('Found all reviews in the search_years range!')
                    return


def parse_all_reviews_otzovik(output_file: Dict[str, Any], org_name: str,
                              links_to_parse: list):
    """
    Collects otzovik.com reviews by iterating over a list of links.
    Saves the information to a file.

    Args:
        output_file: the name of the file in which the intermediate result
            of executing the function is written
        org_name: organization or product name
        links_to_parse: a list of all links that needs to be parsed

    Returns:
        None (writes the results to a file)
    """
    with open(output_file['name'], 'a', encoding='utf-8', newline='') as file:
        writer = csv.DictWriter(file, delimiter='\t',
                                fieldnames=output_file['fields'])
        for iter_link in tqdm(links_to_parse):
            # print(f'Processing {iter_link}...')
            driver.get(iter_link)
            if 'capt4a' in driver.current_url:
                input('Press ENTER after filling CAPTCHA')
            else:
                sleep(2)

            # метаинформация
            title = driver.find_element(
                'css selector', 'div.review-wrap').find_element(
                'tag name', 'h1').text
            date = driver.find_element(
                'css selector', 'span.review-postdate.dtreviewed').text
            author = driver.find_element(
                'css selector', 'a.user-login.fit-with-ava.url.fn').text
            rating = float(driver.find_element(
                'css selector', 'div.rating-score.tooltip-right').text)
            helpful_count = int(driver.find_element(
                'css selector', 'span.review-btn.review-yes.tooltip-top').text)

            # текст
            text = driver.find_element(
                'css selector', 'div.review-plus'
            ).text + ' ' + driver.find_element(
                'css selector', 'div.review-minus'
            ).text + ' ' + driver.find_element(
                'css selector', 'div.review-body.description').text

            # ответ организации
            response, response_date = np.nan, np.nan
            try:
                thread = driver.find_elements(
                    'css selector', 'div.comment-thread')
                for comm in thread:
                    comm_author = comm.find_element(
                        'css selector', 'a.user-login').text
                    if org_name in comm_author or comm_author in org_name:
                        response = comm.find_element(
                            'css selector', 'div.comment-body').text
                        response_date = comm.find_element(
                            'css selector', 'div.comment-postdate.ts').text
            except NoSuchElementException:
                pass

            writer.writerow({
                'download_date': datetime.today(),
                'source': 'https://otzovik.com/',
                'category': 'отзыв',
                'product': driver.title,
                'url': iter_link,
                'date': date,
                'product_name': title,
                'author': author,
                'text': text,
                'response': response,
                'response_date': response_date,
                'rating': rating,
                'helpful_count': helpful_count,
                'address': np.nan})


def parse_otzovik_com(url: str, org_name: str, search_years: Set[int]):
    """
    Collects all otzovik.com reviews, saving the intermediate and final results
    to two files.

    Args:
        url: the url from which the search starts
        org_name: name of the company or product to search for
        search_years: set of years for which the search is carried out

    Returns:
        pd.DataFrame with reviews
    """
    if not os.path.exists('res/otzovik'):
        os.makedirs('res/otzovik')

    # собираем ссылки с сохранением в файл
    collect_otzovik_links(url, search_years)

    # считываем ссылки
    with open('res/otzovik/otzovik_links.txt', 'r', encoding='utf-8') as f_in:
        links_to_parse = list_unique(
            [link.strip() for link in f_in.readlines()])

    # создаем файл с заголовками и обновляем список ссылок
    out_file_info, new_links_to_parse = create_output_file(
        'res/otzovik/otzovik_reviews.tsv', links_to_parse)
    print(f'Found {len(new_links_to_parse)} links to parse.')

    # собираем и сохраняем отзывы
    parse_all_reviews_otzovik(out_file_info, org_name, new_links_to_parse)
    otzovik_df = pd.read_csv('res/otzovik/otzovik_reviews.tsv', sep='\t')
    otzovik_df = format_and_save_dataframe(otzovik_df, search_years)

    return otzovik_df


def parse_plaso_pro(
        url: str, search_years: Set[int], address: str) -> pd.DataFrame:
    """
    Collects reviews from plaso.pro and saves them to a pd.DataFrame.

    Args:
        url: page with reviews
        search_years: a set of years which are of interest
        address: adress of the organisation

    Returns:
        pd.DataFrame with reviews
    """
    driver.get(url)
    reviews = driver.find_elements(By.CLASS_NAME, 'review_part')

    all_reviews = []
    for review in reviews:
        date = review.find_element(By.CLASS_NAME, 'review_time').text
        author = review.find_element(By.CLASS_NAME, 'font-weight-bold').text
        text = review.find_element(By.CLASS_NAME, 'review_text').text
        rating = len(review.find_elements(By.CLASS_NAME, 'yellow'))
        all_reviews.append({'download_date': datetime.today(),
                            'source': 'https://plaso.pro/',
                            'category': 'отзыв',
                            'product': driver.title,
                            'url': driver.current_url,
                            'date': date,
                            'product_name': np.nan,
                            'author': author,
                            'text': text,
                            'response': np.nan,
                            'response_date': np.nan,
                            'rating': rating,
                            'helpful_count': np.nan,
                            'address': address})
    plaso_df = pd.DataFrame(all_reviews)
    plaso_df = format_and_save_dataframe(plaso_df, search_years)

    return plaso_df


def parse_gov_zakupki_and_tendery(
        url: str, start_page: int, end_page: int, step: int,
        search_years: Set[int]) -> pd.DataFrame:
    """
    Collects reviews from gov-zakupki.ru or tendery.ru and saves them to a file.

    Args:
        url: url to the page with reviews
        start_page: the first page to collect reviews from
        end_page: the last page to collect reviews from
        step: the step in page numbering
        search_years: set of years for which the search is carried out

    Returns:
        pd.DataFrame with reviews

    Raises:
        ValueError if the source is given incorrectly
    """
    all_reviews = []
    for i in tqdm(range(start_page, end_page + step, step)):
        if url.startswith('http://forum.gov-zakupki.ru/'):
            page_url = f'{url}{i}.html'
        elif url.startswith('http://www.tendery.ru/'):
            page_url = url + str(i)
        else:
            raise ValueError('Этот сайт не поддерживается')
        driver.get(page_url)
        for review in driver.find_elements(
                'css selector', 'div.postbody')[0:-1]:
            post_info = review.find_element('css selector', 'p.author')
            date = post_info.text.split('»')[-1].strip()
            author = post_info.find_elements('tag name', 'a')[-1].text
            full_text = review.find_element('css selector', 'div.content').text
            try:
                blockquote = review.find_element('tag name', 'blockquote').text
            except NoSuchElementException:
                blockquote = ''
            all_reviews.append({
                'download_date': datetime.today(),
                'source': re.search(r'http://.+?/', url).group(),
                'category': 'форум',
                'product': driver.title,
                'url': page_url,
                'date': date,
                'product_name': driver.find_element(
                    'css selector', 'h3.first').text,
                'author': author,
                'text': full_text.replace(blockquote, '').strip(),
                'response': np.nan,
                'response_date': np.nan,
                'rating': np.nan,
                'helpful_count': np.nan,
                'address': np.nan})
        sleep(1)

    review_df = pd.DataFrame(all_reviews)
    review_df = format_and_save_dataframe(review_df, search_years)

    return review_df


def parse_app_store(app_id: [str, int], country: str,
                    search_years: Set[int]) -> pd.DataFrame:
    """
    Collects App Store reviews.

    Args:
        app_id: app id
        country: country name
        search_years: set of years for which the search is carried out

    Returns:
        pd.DataFrame with reviews
    """
    url = f'https://itunes.apple.com/{country}/rss/customerreviews/' \
          f'id={app_id}/sortBy=mostRecent/json'
    response = requests.get(url, timeout=120)
    if response.status_code != 200:
        raise ValueError('Bad response code')

    data = response.json()
    reviews = []
    for entry in data.get('feed', {}).get('entry', [])[1:]:
        reviews.append({
            'download_date': datetime.today(),
            'source': 'https://apps.apple.com/ru/',
            'category': 'отзыв',
            'product': np.nan,
            'url': f'https://apps.apple.com/ru/app'
                   f'/самокат-доставка-продуктов/id{app_id}',
            'date': str(entry.get('updated', {}).get(
                'label')).split('T', maxsplit=1)[0],
            'product_name': entry.get('product_name', {}).get('label'),
            'author': entry.get('author', {}).get('name', {}).get('label'),
            'text': entry.get('content', {}).get('label'),
            'response': np.nan,
            'response_date': np.nan,
            'rating': float(entry.get('im:rating', {}).get('label')),
            'helpful_count': np.nan,
            'address': np.nan})

    app_store_df = pd.DataFrame(reviews)
    app_store_df = format_and_save_dataframe(app_store_df, search_years)
    return app_store_df


# def parse_app_store(url: str, search_years: Set[int]) -> pd.DataFrame:
#     """
#     Collects App Store reviews.
#
#     Args:
#         url: a url to the https://apps.apple.com/ru/ website page to be parsed
#         search_years: a set of years which are of interest
#
#     Returns:
#         pd.DataFrame with reviews
#     """
#     name = str(re.search(r'https://apps\.apple\.com/ru/app/(.*)/id(.*)',
#                          url).group(1))
#     a_id = str(re.search(r'https://apps\.apple\.com/ru/app/(.*)/id(.*)',
#                          url).group(2))
#     app_store_revs = AppStore(country='ru', app_name=name, app_id=a_id)
#     app_store_revs.review()
#     app_store_list = []
#     for item in app_store_revs.reviews:
#         app_store_list.append({
#             'download_date': datetime.today(),
#             'source': 'https://apps.apple.com/ru/',
#             'category': 'отзыв',
#             'product': driver.product_name,
#             'url': 'https://apps.apple.com/ru/app/' + name + '/id' + a_id,
#             'date': item['date'],
#             'product_name': item['product_name'],
#             'author': item['userName'],
#             'text': item['review'],
#             'response': np.nan,
#             'response_date': np.nan,
#             'rating': float(item['rating']),
#             'helpful_count': np.nan,
#             'address': np.nan})
#
#     app_store_df = pd.DataFrame(app_store_list)
#     app_store_df = format_and_save_dataframe(app_store_df, search_years)
#
#     return app_store_df


def parse_rustore_page(url: str) -> List[Dict[str, Any]]:
    """
    Collects reviews from one RuStore page.

    Args:
        url: url to the page with reviews

    Returns:
        list of dicts with review information
    """
    driver.get(url)
    sleep(3)
    review_list = driver.find_elements('css selector', 'li.k4E0naQ0')
    if review_list:
        reviews = chair_one(review_list, url)
    else:
        review_list = driver.find_elements(
            'css selector', 'div.Z8HHraBe.JjR_4jS4.Q_8seuPA')
        reviews = chair_two(review_list, url)
    return reviews


def chair_one(review_list: List[WebElement], url: str) -> List[Dict[str, Any]]:
    """
    Parser for the first version of HTML structure.

    Args:
        review_list: list of WebElements with reviews
        url: url to the webpage with reviews

    Returns:
        list of dicts with review information
    """
    reviews = []
    for review in review_list:
        likes = review.find_elements('css selector', 'div.YkLCvUyi')[0].text
        likes = int(re.search(r'\d+', likes).group())
        rating = review.find_element(
            'css selector', 'span.T9nXRR91').find_element(
            'tag name', 'stop').get_attribute('offset')
        rev_parts = review.find_elements('css selector', 'p._1jgKerfF')
        response_date, response_text = np.nan, np.nan
        if len(rev_parts) > 1:
            response_date = review.find_elements(
                'css selector', 'p.qPdEbUHn')[-1].text
            response_text = rev_parts[-1].text
        reviews.append({'download_date': datetime.today(),
                        'source': 'https://apps.rustore.ru/',
                        'category': 'отзыв',
                        'product': np.nan,
                        'url': url,
                        'date': review.find_element(
                            'css selector', 'p.qPdEbUHn').text,
                        'product_name': np.nan,
                        'author': review.find_element(
                            'css selector', 'h3.KiXqNJ3O.c0TuZspB').text,
                        'text': review.find_element(
                            'css selector', 'p._1jgKerfF').text,
                        'response': response_text,
                        'response_date': response_date,
                        'rating': rating,
                        'helpful_count': int(likes),
                        'address': np.nan})
    return reviews


def chair_two(review_list: List[WebElement], url: str) -> List[Dict[str, Any]]:
    """
    Parser for the second version of HTML structure.

    Args:
        review_list: list of WebElements with reviews
        url: url to the webpage with reviews

    Returns:
        list of dicts with review information
    """
    reviews = []
    for review in review_list:
        likes = review.find_element('css selector', 'div.evliIl7Y').text
        likes = int(re.search(r'\d+', likes).group())
        rating_parts = review.find_elements('css selector', 'svg.BzjMd7T0')
        rating = 0
        for part in rating_parts:
            if 'star_new_full' in part.find_element(
                    'tag name', 'use').get_attribute('href'):
                rating += 1
            else:
                break
        rev_parts = review.find_elements('css selector', 'p.YOMjlbtO')
        response_date, response_text = np.nan, np.nan
        if len(rev_parts) > 1:
            response_date = review.find_elements(
                'css selector', 'span.X7K5JxYD')[-1].text
            response_text = rev_parts[-1].text
        reviews.append({'download_date': datetime.today(),
                        'source': 'https://apps.rustore.ru/',
                        'category': 'отзыв',
                        'product': driver.find_element(
                            'css selector',
                            'h2.mVjlJ2jL.VV0MZngj.S_n__8iC').text,
                        'url': url,
                        'date': review.find_element(
                            'css selector', 'span.X7K5JxYD').text,
                        'product_name': np.nan,
                        'author': review.find_element(
                            'css selector', 'span.giIK213i').text,
                        'text': review.find_element(
                            'css selector', 'p.YOMjlbtO').text,
                        'response': response_text,
                        'response_date': response_date,
                        'rating': rating,
                        'helpful_count': int(likes),
                        'address': np.nan})
    return reviews


def parse_all_rustore_pages(url: str) -> List[Dict[str, Any]]:
    """
    Collects reviews from all RuStore pages.

    Args:
        url: url to the page with reviews

    Returns:
        list of dicts with review information
    """
    all_reviews = []
    driver.get(url)
    WebDriverWait(driver, 5).until(
        ec.presence_of_element_located(('css selector', 'a.PhzrBgjj')))
    last_page = int(driver.find_elements('css selector', 'a.PhzrBgjj')[-1].text)
    for i in tqdm(range(last_page)):
        curr_url = url + f'/page-{i+1}'
        all_reviews.extend(parse_rustore_page(curr_url))
    return all_reviews


def parse_rustore(url: str, search_years: Set[int]) -> pd.DataFrame:
    """
    Collects reviews for a RuStore app and saves them to a file.

    Args:
        url: url to the page with reviews
        search_years: set of years for which the search is carried out

    Returns:
        pd.DataFrame with reviews
    """
    all_reviews = parse_all_rustore_pages(url)
    rustore_df = pd.DataFrame(all_reviews)
    rustore_df = format_and_save_dataframe(rustore_df, search_years)
    return rustore_df


def find_all_ru_play_market_posts(
        url: str) -> Tuple[List[WebElement], List[WebElement]]:
    """
    Collects all posts from a RuPlayMarket review page and sorts them into
    customer reviews and company replies.

    Args:
        url: url to the page with reviews

    Returns:
        two lists: one with reviews and one with replies
    """
    driver.get(url)
    posts = driver.find_element(
        'class name', 'reviews__content-list').find_elements(
        'class name', 'text-more-wrapper')
    reviews, answers = [], []
    for i, post in enumerate(posts):
        classes = set(post.get_attribute('class').split())
        if 'app-comment__wrapper' not in classes:
            reviews.append(post)
            try:
                next_classes = set(posts[i + 1].get_attribute('class').split())
                if 'app-comment__wrapper' in next_classes:
                    answers.append(posts[i + 1])
                else:
                    answers.append(np.nan)
            except IndexError:
                answers.append(np.nan)
    return reviews, answers


def collect_ru_play_market_reviews(url: str) -> List[Dict[str, Any]]:
    """
    Collects review information from RuPlayMarket.

    Args:
        url: url to the page with reviews

    Returns:
        list of dicts with review information
    """
    reviews, answers = find_all_ru_play_market_posts(url)
    review_list = []
    for i, review in enumerate(reviews):
        try:
            review.find_element('class name', 'button-text-more').click()
            sleep(1)
        except NoSuchElementException:
            pass
        author = review.find_element(
            'class name', 'app-comment__author-name').text.strip()
        date = review.find_element(
            'class name', 'app-comment__date').text.strip()
        text = review.find_element(
            'class name', 'app-comment__body').text.strip()
        stars = review.find_elements(
            'css selector', 'span.app-summary__star > img')
        srcs = Counter([star.get_attribute('src') for star in stars])
        rating = srcs[
            'https://cdn.ruplay.market/img/rating/star-colored.svg?v=20230620']
        if not pd.isnull(answers[i]):
            try:
                answers[i].find_element(
                    'class name', 'button-text-more').click()
                sleep(1)
            except NoSuchElementException:
                pass
            response_text = answers[i].find_element(
                'class name', 'app-comment__body').text.strip()
        else:
            response_text = np.nan
        review_list.append({'download_date': datetime.today(),
                            'source': 'https://ruplay.market/',
                            'category': 'отзыв',
                            'product': driver.title,
                            'url': url,
                            'date': date,
                            'product_name': np.nan,
                            'author': author,
                            'text': text,
                            'response': response_text,
                            'response_date': np.nan,
                            'rating': rating,
                            'helpful_count': np.nan,
                            'address': np.nan})
    return review_list


def parse_ru_play_market(url: str, search_years: Set[int]) -> pd.DataFrame:
    """
    Collects reviews from RuPlayMarket and saves them to a file.

    Args:
        url: url to the page with reviews
        search_years: set of years for which the search is carried out

    Returns:
        pd.DataFrame with reviews
    """
    all_reviews = collect_ru_play_market_reviews(url)
    ruplay_df = pd.DataFrame(all_reviews)
    ruplay_df = format_and_save_dataframe(ruplay_df, search_years)
    return ruplay_df


def load_app_gallery_reviews(url: str) -> List[Dict[str, Any]]:
    """
    Loads the AppGallery review page and collects review information.

    Args:
        url: url to the page with reviews

    Returns:
        list of dicts with review information
    """
    driver.get(url)
    sleep(5)
    driver.find_element('class name', 'more').click()
    input('Scroll down manually, then press ENTER')
    reviews = driver.find_elements('class name', 'comment_item')
    print(len(reviews))
    all_reviews = []
    for review in reviews:
        date = review.find_element('css selector',
                                   'div.deviceName > div.right').text
        author = review.find_element('class name', 'userName').text
        text = review.find_element('class name', 'part_middle').text
        stars = review.find_elements('css selector', 'div.newStarBox > img')
        srcs = Counter([star.get_attribute('src')[-9:] for star in stars])
        rating = srcs['8L3N2Zz4=']
        all_reviews.append({'download_date': datetime.today(),
                            'source': 'https://appgallery.huawei.ru/',
                            'category': 'отзыв',
                            'product': driver.find_elements(
                                'css selector', 'div.product_name')[1].text,
                            'url': url,
                            'date': date,
                            'product_name': np.nan,
                            'author': author,
                            'text': text,
                            'response': np.nan,
                            'response_date': np.nan,
                            'rating': rating,
                            'helpful_count': np.nan,
                            'address': np.nan})
    return all_reviews


def parse_app_gallery(url: str, search_years: Set[int]) -> pd.DataFrame:
    """
    Collects reviews from AppGallery and saves them to a file.

    Args:
        url: url to the page with reviews
        search_years: set of years for which the search is carried out

    Returns:
        pd.DataFrame with reviews
    """
    all_reviews = load_app_gallery_reviews(url)
    appgallery_df = pd.DataFrame(all_reviews)
    appgallery_df = format_and_save_dataframe(appgallery_df, search_years)
    return appgallery_df


def iterate_over_galaxy_reviews(url: str, reviews_list: List[WebElement],
                                answers_list: List[Union[WebElement, float]]
                                ) -> List[Dict[str, Any]]:
    """
    Collects reviews from Galaxy Store and saves them to a list.

    Args:
        url: url to the page with reviews
        reviews_list: list with customers' reviews
        answers_list: list with developer's answers to reviews

    Returns:
        list of dicts with review information
    """
    all_reviews = []
    for i, review in tqdm(enumerate(reviews_list), total=len(reviews_list)):
        date = review.find_element('tag name', 'figcaption').text
        author = review.find_element('tag name', 'em').text
        text = review.find_element('class name',
                                   'CustomerReview_content_review__3HHFp').text
        style = review.find_element(
            'class name', 'RatingStar_star__1Va97').get_attribute('style')
        rating_raw = re.search(r'background-size: 16px; width: (.*?)%;',
                               style).group(1)
        rating = (float(rating_raw) - 1) / 20
        response, response_date = np.nan, np.nan
        if not pd.isnull(answers_list[i]):
            response = answers_list[i].find_element(
                'class name', 'CustomerReview_content_answer__2XEpH').text
            response_date = answers_list[i].find_element(
                'class name', 'CustomerReview_answer_date__kd63S').text
        all_reviews.append({'download_date': datetime.today(),
                            'source': 'https://galaxystore.samsung.com/',
                            'category': 'отзыв',
                            'product': driver.title,
                            'url': url,
                            'date': date,
                            'author': author,
                            'product_name': np.nan,
                            'text': text,
                            'response': response,
                            'response_date': response_date,
                            'rating': rating,
                            'helpful_count': np.nan,
                            'address': np.nan})
    return all_reviews


def load_galaxy_page(url: str) -> List[Dict[str, Any]]:
    """
    Opens comment section, scrolls down to show all reviews, and collects review
    information.

    Args:
        url: url to the page with reviews

    Returns:
        list of dicts with review information
    """
    driver.get(url)
    sleep(1)
    driver.find_elements('class name', 'Tabs_tab_list_item__2HfqW')[1].click()
    sleep(1)
    while True:
        scroll_down_simple()
        try:
            driver.find_element('class name',
                                'CustomReviewContainer_morereview_btn__3xmKM'
                                ).click()
            sleep(1)
        except NoSuchElementException:
            break
    galaxy_page = driver.find_element(
        'class name', 'CustomReviewContainer_review_content__3Ma9f')
    posts = galaxy_page.find_elements('xpath', './div')
    reviews, answers = [], []
    for i, post in enumerate(posts):
        classes = set(post.get_attribute('class').split())
        if 'CustomerReview_customer_review__qmUeI' in classes:
            reviews.append(post)
            try:
                next_post = posts[i + 1]
                answers.append(next_post.find_element(
                    'class name', 'CustomerReview_seller_answer__1SbB-'))
            except (IndexError, NoSuchElementException):
                answers.append(np.nan)
    galaxy_list = iterate_over_galaxy_reviews(url, reviews, answers)
    return galaxy_list


def parse_galaxy_store(url: str, search_years: Set[int]) -> pd.DataFrame:
    """
    Collects reviews from Galaxy Store and saves them to a file.

    Args:
        url: url to the page with reviews
        search_years: set of years for which the search is carried out

    Returns:
        pd.DataFrame with reviews
    """
    all_reviews = load_galaxy_page(url)
    galaxy_df = pd.DataFrame(all_reviews)
    galaxy_df = format_and_save_dataframe(galaxy_df, search_years)
    return galaxy_df


def get_vk_profiles(
        profiles: Dict[Any, Any], groups: Dict[Any, Any]) -> Dict[int, str]:
    """
    Collects information about post/comment authors.

    Args:
        profiles: dict with profile information
        groups: dict with group information

    Returns:
        dict with profile information
    """
    profile_info = {profile['id']: ' '.join([profile['first_name'],
                                             profile['last_name']])
                    for profile in profiles}
    for group in groups:
        profile_info[group['id']] = group['name']
    return profile_info


def get_vk_board_comments(group_id: int, topic_id: int, num_comments: int
                          ) -> List[Dict[Any, Any]]:
    """
    Collects board comments using VK API.
    API also available here: https://dev.vk.com/method/board.getComments

    Args:
        group_id: id of the group where the topic can be found
        topic_id: id of the topic
        num_comments: total number of comments in the topic

    Returns:
        list of dicts with comment information
    """
    offset = 0
    all_comments = []
    for _ in range(math.ceil(num_comments/100)):
        # pylint: disable=no-member
        all_comments.append(
            api.board.getComments(v='5.131',
                                  group_id=group_id,
                                  topic_id=topic_id,
                                  need_likes=1,
                                  offset=offset,
                                  count=100,
                                  extended=1,
                                  sort='desc'))
        offset += 100
    return all_comments


def parse_vk_board_comments(board_link: str, group_id: int, topic_id: int,
                            num_comments: int) -> List[Dict[str, Any]]:
    """
    Collects all vk.com board comments.

    Args:
        board_link: url to the board with comments
        group_id: id of the group where the topic can be found
        topic_id: id of the topic
        num_comments: total number of comments in the topic

    Returns:
        list of dicts with comment information
    """
    all_comment_info = get_vk_board_comments(group_id, topic_id, num_comments)
    parsed_comments = []
    for comm_list in all_comment_info:
        profile_info = get_vk_profiles(
            comm_list['profiles'], comm_list['groups'])
        for comment in comm_list['items']:
            parsed_comments.append({
                'download_date': datetime.today(),
                'source': 'https://vk.com/',
                'category': 'сообщение в обсуждении',
                'product': np.nan,
                'url': board_link,
                'date': comment['date'],
                'product_name': np.nan,
                'author': profile_info[abs(comment['from_id'])],
                'text': comment['text'],
                'response': np.nan,
                'response_date': np.nan,
                'rating': np.nan,
                'helpful_count': comment['likes']['count'],
                'address': np.nan})
    return parsed_comments


def get_vk_wall_posts(owner_id: int, num_posts: int = 100,
                      query: Optional[str] = None) -> List[Dict[Any, Any]]:
    """
    Finds all wall posts matching a query.

    Args:
        owner_id: id of the wall owner (must be negative if owner is a group)
        num_posts: number of posts to collect
        query: search query, if necessary

    Returns:
        dict with posts information
    """
    offset = 0
    all_posts = []
    for _ in range(math.ceil(num_posts / 100)):
        if query:
            # pylint: disable=no-member
            all_posts.append(api.wall.search(v=5.199,
                                             owner_id=owner_id,
                                             query=query,
                                             offset=offset,
                                             count=100,
                                             extended=1))
        else:
            all_posts.append(api.wall.get(v=5.199,  # pylint: disable=no-member
                                          owner_id=owner_id,
                                          offset=offset,
                                          count=100,
                                          extended=1))
        offset += 100
        sleep(1.5)
    return all_posts


def parse_vk_wall_posts(owner_id: int, count: int = 100,
                        query: Optional[str] = None) -> list[dict]:
    """
    Collects information about wall posts matching a query.

    Args:
        owner_id: id of the wall owner (must be negative if owner is a group)
        count: number of posts to collect
        query: search query, if necessary

    Returns:
        list of dicts with posts information
    """
    print('Parsing wall posts...')
    post_info = []
    posts = get_vk_wall_posts(owner_id, count, query)
    print(f'Found {len(posts)} batches, 100 posts each.')
    for batch in posts:
        profile_info = get_vk_profiles(batch['profiles'], batch['groups'])
        for item in batch['items']:
            post_id = item['id']
            post_info.append(
                {'download_date': datetime.now(),
                 'source': 'https://vk.com/',
                 'category': 'запись на стене',
                 'product': np.nan,
                 'url': f'https://vk.com/wall{owner_id}_{post_id}',
                 'date': item['date'],
                 'product_name': np.nan,
                 'author': profile_info[abs(item['from_id'])],
                 'text': item['text'],
                 'response': np.nan,
                 'response_date': np.nan,
                 'rating': np.nan,
                 'helpful_count': item['likes']['count'],
                 'post_id': post_id,
                 'address': np.nan})
    return post_info


def get_vk_wall_comments_batch(owner_id: int, post_id: int,
                               offset: int, count: int) -> dict:
    """
    Collect one batch of comments from a vk.com wall post.

    Args:
        owner_id: wall owner (must be negative if owner is a group)
        post_id: post id
        offset: the first comment to collect
        count: number of comments to collect

    Returns:
        dict with comments information
    """
    sleep(1.5)
    # pylint: disable=no-member
    return api.wall.getComments(v=5.199,
                                owner_id=owner_id,
                                post_id=post_id,
                                offset=offset,
                                count=count,
                                need_likes=1,
                                extended=1)


def get_all_vk_wall_comments(owner_id: int, post_id: int, offset: int = 0,
                             count: int = 100) -> list[dict]:
    """
    Collects all comments from a vk.com wall post.

    Args:
        owner_id: wall owner (must be negative if owner is a group)
        post_id: post id
        offset: the first comment to collect
        count: number of comments to collect

    Returns:
        list of dicts with comments information
    """
    # print('Parsing comments to wall posts...')
    first_thread = get_vk_wall_comments_batch(owner_id, post_id, offset, count)
    num_comments = first_thread['count']
    all_threads = [first_thread]
    if num_comments <= 100:
        return all_threads
    offset += 100
    for _ in tqdm(range(math.ceil(num_comments / 100))):
        all_threads.append(get_vk_wall_comments_batch(
            owner_id, post_id, offset, count))
        offset += 100
    print(f'Found {len(all_threads)} comment threads.')
    return all_threads


def get_one_vk_comment_info(comment: dict, profiles: dict, post_id: int,
                            category: str) -> Dict[str, Any]:
    """
    Returns information about one comment.

    Args:
        comment: dict with full comment information
        profiles: profile information
        post_id: post id
        category: category of the comment

    Returns:
        dict with key comment information
    """
    return {'download_date': datetime.now(),
            'source': 'https://vk.com/',
            'category': category,
            'product': np.nan,
            'url': 'https://vk.com/wall' +
                   f'{comment["owner_id"]}_{comment["post_id"]}',
            'date': comment['date'],
            'author': profiles[abs(comment['from_id'])],
            'text': comment['text'],
            'helpful_count': comment['likes']['count'],
            'post_id': post_id,
            'address': np.nan}


def get_info_from_vk_comments(owner_id: int, post_id: int, offset: int = 0,
                              count: int = 100) -> list[dict]:
    """
    Collects information about all comments to one vk.com wall post.

    Args:
        owner_id: wall owner (must be negative if owner is a group)
        post_id: post id
        offset: the first comment to collect
        count: number of comments to collect

    Returns:
        list of dicts with comments information
    """
    all_threads = get_all_vk_wall_comments(owner_id, post_id, offset, count)
    all_comments = []
    for thread in all_threads:
        items = thread['items']
        profile_info = get_vk_profiles(thread['profiles'], thread['groups'])
        for item in items:
            if 'deleted' in item:
                if item['deleted']:
                    continue
            all_comments.append(get_one_vk_comment_info(
                item, profile_info, post_id, 'комментарий к записи'))
            if item['thread']['count'] > 0:
                # pylint: disable=no-member
                replies = api.wall.getComments(v=5.199,
                                               owner_id=owner_id,
                                               post_id=post_id,
                                               offset=0,
                                               count=100,
                                               need_likes=1,
                                               extended=1,
                                               comment_id=item['id'])
                reply_profiles = get_vk_profiles(replies['profiles'],
                                                 replies['groups'])
                sleep(1.5)
                for reply in replies['items']:
                    all_comments.append(get_one_vk_comment_info(
                        reply, reply_profiles, post_id, 'ответ на комментарий'))
    return all_comments


def parse_vk_com(
        search_type: Literal['board comments', 'posts with comments'],
        search_years: Set[int], **kwargs) -> pd.DataFrame:
    """
    Collects information from vk.com by calling functions in vk_parser.

    Args:
        search_type: type of search to be carried out
        search_years: set of years for which the search is carried out
        **kwargs: arguments for vk_parser

    Returns:
        pd.DataFrame with results
    """
    vk_data = []
    if search_type == 'board comments':
        vk_data = parse_vk_board_comments(board_link=kwargs['board_link'],
                                          group_id=kwargs['group_id'],
                                          topic_id=kwargs['topic_id'],
                                          num_comments=kwargs['num_comments'])
    elif search_type == 'posts with comments':
        if 'query' in kwargs:
            posts = parse_vk_wall_posts(
                owner_id=kwargs['owner_id'], query=kwargs['query'],
                count=kwargs['count'])
        else:
            posts = parse_vk_wall_posts(
                owner_id=kwargs['owner_id'], count=kwargs['count'])
        comments = []
        for post in tqdm(posts):
            comments.extend(get_info_from_vk_comments(
                owner_id=kwargs['owner_id'], post_id=post['post_id']))
        vk_data = posts + comments
    vk_df = format_and_save_dataframe(pd.DataFrame(vk_data), search_years)
    return vk_df


def collect_myfin_reviews(start_link: str, num_pages: int
                          ) -> List[Dict[str, Any]]:
    """
    Loads and parses reviews from myfin.by, iterating over num_pages.

    Args:
        start_link: first page with reviews
        num_pages: number of pages

    Returns:
        list of dicts with review information
    """
    all_reviews = []
    for i in tqdm(range(num_pages+1)):
        url = start_link + '?page=' + str(i+1)
        driver.get(url)
        for review in driver.find_element(
                'css selector', 'div.items').find_elements('css selector',
                                                           'div.item'):
            date = review.find_element(
                'css selector', 'time.date').get_attribute('datetime')
            author = review.find_element(
                'css selector', 'div.name').text.strip()
            text = review.find_element(
                'css selector', 'div.question').text.strip()
            rating_style = review.find_element(
                'css selector', 'div.mainrating').get_attribute('style')
            rating = int(re.search(r'\d+', rating_style).group()) / 16
            response, response_date = np.nan, np.nan
            try:
                resp_cont = review.find_element('css selector', 'div.answer')
                response = '\n'.join(resp_cont.text.strip().split('\n')[1:])
                response_date = resp_cont.find_element(
                    'css selector', 'time.answer_data').get_attribute(
                    'datetime')
            except NoSuchElementException:
                pass
            all_reviews.append({'download_date': datetime.now(),
                                'source': 'https://myfin.by/',
                                'category': 'отзыв',
                                'product': driver.title,
                                'url': url,
                                'date': date,
                                'product_name': np.nan,
                                'author': author,
                                'text': text,
                                'response': response,
                                'response_date': response_date,
                                'rating': rating,
                                'helpful_count': np.nan,
                                'address': np.nan})
    return all_reviews


def parse_myfin_by(start_link: str, num_pages: int,
                   search_years: Set[int]) -> pd.DataFrame:
    """
    Collects reviews from myfin.by and saves them to a file.

    Args:
        start_link: start page with reviews
        num_pages: number of pages to parse
        search_years: set of years for which the search is carried out

    Returns:
        pd.DataFrame with reviews
    """
    all_reviews = collect_myfin_reviews(start_link, num_pages)
    myfin_df = pd.DataFrame(all_reviews)
    myfin_df = format_and_save_dataframe(myfin_df, search_years)
    return myfin_df


def collect_retwork_reviews(url: str) -> List[Dict[str, Any]]:
    """
    Loads and parses reviews from retwork.com.

    Args:
        url: page with reviews

    Returns:
        list of dicts with review information
    """
    driver.get(url)
    scroll_down_simple()
    all_reviews = []
    for review in driver.find_element('tag name', 'index').find_elements(
            'css selector', 'div.item'):
        driver.execute_script("arguments[0].scrollIntoView(true);", review)
        try:
            address = review.find_element(
                'css selector', 'p.company_addres').text
        except NoSuchElementException:
            address = ''
        try:
            review.find_element('css selector', 'a.btn_ext').click()
            sleep(1)
        except ElementNotInteractableException:
            pass
        rev_url = review.find_element('tag name', 'a').get_attribute('href')
        date = review.find_element(
            'css selector', 'div.date').text.split('|')[0].strip()
        helpful_count = int(
            review.find_element('css selector', 'div.truefalse'
                                ).find_element('tag name', 'span').text)
        rating = len(
            review.find_elements('css selector', 'span.fa.icon-star-5'))
        title = review.find_element('css selector', 'a.product_name').text
        text = review.find_element('css selector', 'div.body').text.split(
            '<< Свернуть')[1].strip()
        parts = text.split('\nСсылка на страницу:')
        if len(parts) > 1:
            text = ''.join(parts[0:-1])
        else:
            text = parts[0]
        text = text.replace(address, '').strip()
        all_reviews.append({'download_date': datetime.now(),
                            'source': 'https://retwork.com/',
                            'category': 'отзыв',
                            'product': driver.title,
                            'url': rev_url,
                            'date': date,
                            'product_name': title,
                            'author': np.nan,
                            'text': text,
                            'response': np.nan,
                            'response_date': np.nan,
                            'rating': rating,
                            'helpful_count': helpful_count,
                            'address': address})
    return all_reviews


def parse_retwork_com(url: str, search_years: Set[int]) -> pd.DataFrame:
    """
    Collects reviews from retwork.com and saves them to a file.

    Args:
        url: page with reviews
        search_years: set of years for which the search is carried out

    Returns:
        pd.DataFrame with reviews
    """
    all_reviews = collect_retwork_reviews(url)
    retwork_df = pd.DataFrame(all_reviews)
    retwork_df = format_and_save_dataframe(retwork_df, search_years)
    return retwork_df


def collect_etorazvod_reviews(start_link: str) -> List[Dict[str, Any]]:
    """
    Collects information about etorazvod.ru reviews.

    Args:
        start_link: page with reviews

    Returns:
        list of dicts with review information
    """
    # открываем сайт, кликаем на отзывы
    driver.get(start_link)
    buttons = driver.find_elements(
        'css selector', 'a.color_dark_gray.link_no_underline')[1]
    buttons[1].click()
    sleep(2)

    # собираем ссылки на отзывы
    reviews = driver.find_element(
        'css selector', 'ul.show-comments').find_elements(
        'css selector', 'div.comment-body')
    rev_links = [review.find_element(
        'css selector', 'a.review_title.link_no_underline'
    ).get_attribute('href')for review in reviews]

    # переключаемся на жалобы
    buttons[2].click()
    sleep(2)

    # собираем жалобы
    all_reviews = []
    for complaint in driver.find_element(
            'css selector', 'ul#abuses').find_elements(
            'css selector', 'div.comment-body'):
        comp_id = re.search(r'\d+', complaint.get_attribute('class')).group()
        date = complaint.find_element(
            'css selector', 'span.comment-date').get_attribute(
            'content').split('T')[0]
        author = complaint.find_element(
            'css selector', 'span.comment-author').text
        text = complaint.find_element('css selector', 'div.comment_text').text
        all_reviews.append({'download_date': datetime.now(),
                            'source': 'https://etorazvod.ru/',
                            'category': 'жалоба',
                            'product': driver.title,
                            'url': f'{start_link}comment-{comp_id}/',
                            'date': date,
                            'product_name': np.nan,
                            'author': author,
                            'text': text,
                            'response': np.nan,
                            'response_date': np.nan,
                            'rating': np.nan,
                            'helpful_count': np.nan,
                            'address': np.nan})

    # проходим по отзывам и собираем информацию
    for url in rev_links:
        print(f'Parsing {url}...')
        driver.get(url)
        review = driver.find_element('css selector', 'div.comment-body')
        date = review.find_element(
            'css selector', 'span.comment-date').get_attribute('content')
        title = review.find_element(
            'css selector', 'h1.comment_single_title.summary').text
        author = driver.find_elements(
            'css selector', 'span.comment-author')[-1].text
        text = review.find_element('css selector', 'div.comment_text').text
        rating = int(review.find_element(
            'css selector',
            'div.inner.color_dark_blue.font_bold.font_small.pointer').text)
        helpful_count = int(
            review.find_element('css selector', 'span.number_plus').text)
        all_reviews.append({'download_date': datetime.now(),
                            'source': 'https://etorazvod.ru/',
                            'category': 'отзыв',
                            'product': driver.title,
                            'url': url,
                            'date': date,
                            'product_name': title,
                            'author': author,
                            'text': text,
                            'response': np.nan,
                            'response_date': np.nan,
                            'rating': rating,
                            'helpful_count': helpful_count,
                            'address': np.nan})
    return all_reviews


def parse_etorazvod_ru(start_link: str, search_years: Set[int]) -> pd.DataFrame:
    """
    Collects reviews from etorazvod.ru and saves them to a file.

    Args:
        start_link: page with reviews
        search_years: set of years for which the search is carried out

    Returns:
        pd.DataFrame with reviews
    """
    all_reviews = collect_etorazvod_reviews(start_link)
    etorazvod_df = pd.DataFrame(all_reviews)
    etorazvod_df = format_and_save_dataframe(etorazvod_df, search_years)
    return etorazvod_df


def collect_24_review_one_page(url: str) -> List[Dict[str, Any]]:
    """
    Collects information about 24-review.ru reviews.

    Args:
        url: page with reviews

    Returns:
        list of dicts with review information
    """
    driver.get(url)
    all_reviews = []
    for i, review in enumerate(
            driver.find_elements('css selector', 'div.reviewers-box')):
        date = review.find_elements(
            'xpath', "//meta[contains(@itemprop, 'datePublished')]")[
            i].get_attribute('content')
        author = review.find_elements(
            'xpath', "//span[contains(@itemprop, 'author')]")[i].text
        text = review.find_elements(
            'xpath', "//table[contains(@itemprop, 'description')]")[i].text
        rating = int(review.find_elements(
            'xpath', "//span[contains(@itemprop, 'reviewRating')]")[i].text)
        all_reviews.append({'download_date': datetime.now(),
                            'source': 'https://24-review.ru/',
                            'category': 'отзыв',
                            'product': driver.find_element(
                                'css selector', 'h1.ib').text.replace(
                                'Отзывы о ', ''),
                            'url': url,
                            'date': date,
                            'product_name': np.nan,
                            'author': author,
                            'text': text,
                            'response': np.nan,
                            'response_date': np.nan,
                            'rating': rating,
                            'helpful_count': np.nan,
                            'address': np.nan})
    return all_reviews


def collect_24_review_all_pages(links: List[str]) -> List[Dict[str, Any]]:
    """
    Iterates over all pages with reviews.

    Args:
        links: list of links to parse

    Returns:
        list of dicts with review information
    """
    reviews = []
    for link in tqdm(links):
        reviews.extend(collect_24_review_one_page(link))
    return reviews


def parse_24_review(links: List[str], search_years: Set[int]) -> pd.DataFrame:
    """
    Collects reviews from 24-review.ru and saves them to a file.

    Args:
        links: pages with reviews
        search_years: set of years for which the search is carried out

    Returns:
        pd.DataFrame with reviews
    """
    all_reviews = collect_24_review_all_pages(links)
    rev24_df = pd.DataFrame(all_reviews)
    rev24_df = format_and_save_dataframe(rev24_df, search_years)
    return rev24_df


def collect_kitabi_reviews(url: str) -> List[Dict[str, Any]]:
    """
    Collects information about otzyvy.kitabi.ru reviews.

    Args:
        url: page with reviews

    Returns:
        list of dicts with review information
    """
    driver.get(url)
    input('Press ENTER after filling CAPTCHA')
    driver.find_element('css selector', 'a.global-morelink2').click()
    input('Press ENTER after filling CAPTCHA')

    all_reviews = []
    for review in driver.find_elements(
            'css selector', 'div.reviews-list2-item-container'):
        date = review.find_element(
            'css selector', 'span.reviews-list-item-date').text
        author = review.find_element(
            'css selector', 'span.reviews-list2-item-product_name-name').text
        rating = review.find_element(
            'css selector', 'div.reviews-list2-item-rating').text.strip()
        if rating == 'Хорошо!':
            rating = 5
        else:
            rating = 1
        try:
            more_btn = review.find_element('css selector', 'a.morelink')
            driver.execute_script(
                "arguments[0].scrollIntoView(true);", more_btn)
            sleep(1)
        except NoSuchElementException:
            pass
        text = review.find_element(
            'css selector', 'div.reviews-list2-item-text').text
        all_reviews.append({'download_date': datetime.now(),
                            'source': 'https://otzyvy.kitabi.ru/',
                            'category': 'отзыв',
                            'product': driver.title,
                            'url': url,
                            'date': date,
                            'product_name': np.nan,
                            'author': author,
                            'text': text,
                            'response': np.nan,
                            'response_date': np.nan,
                            'rating': rating,
                            'helpful_count': np.nan,
                            'address': np.nan})
    return all_reviews


def parse_kitabi_ru(url: str, search_years: Set[int]) -> pd.DataFrame:
    """
    Collects reviews from otzyvy.kitabi.ru and saves them to a file.

    Args:
        url: page with reviews
        search_years: set of years for which the search is carried out

    Returns:
        pd.DataFrame with reviews
    """
    all_reviews = collect_kitabi_reviews(url)
    kitabi_df = pd.DataFrame(all_reviews)
    kitabi_df = format_and_save_dataframe(kitabi_df, search_years)
    return kitabi_df


def collect_ru_otzyv_links(start_link: str, num_pages: int):
    """
    Collects links to ru.otzyv.com reviews.

    Args:
        start_link: page with reviews
        num_pages: number of pages to parse

    Returns:
        None (saves links to a file)
    """
    for i in range(num_pages):
        driver.get(f'{start_link}?page={i + 1}')
        input('Press ENTER after filling CAPTCHA')
        with open('res/ruotzyv/ruotzyv_links.txt', 'a', encoding='utf-8'
                  ) as f_out:
            for preview in driver.find_elements('css selector',
                                                'div.comment_row'):
                one_link = preview.find_element(
                    'tag name', 'h2').find_element(
                    'tag name', 'a').get_attribute('href')
                f_out.write(one_link + '\n')


def parse_one_ru_otzyv_page(url: str) -> Dict[str, Any]:
    """
    Collects information about one ru.otzyv.com review.

    Args:
        url: review page

    Returns:
        dict with review information
    """
    driver.get(url)
    input('Press ENTER after filling CAPTCHA')
    title = driver.find_element(
        'tag name', 'h1').text.split('- Отзыв о')[0].strip()
    author = driver.find_element('css selector', 'span.reviewer').text
    date_cont = driver.find_element('css selector', 'div.spec').text
    date = re.search(r', опубликован (.+)', date_cont).group(1)
    desc = driver.find_element('css selector', 'span.comment.description').text
    try:
        advs = driver.find_element('css selector', 'div.advantages').text
        disads = driver.find_element('css selector', 'div.disadvantages').text
    except NoSuchElementException:
        advs, disads = '', ''
    text = '\n'.join([desc, advs, disads]).strip()
    rating_cont = driver.find_element(
        'css selector', 'span.star_rate_big').find_element(
        'tag name', 'span').get_attribute('style')
    rating = int(re.search(r'\d+', rating_cont).group()) / 30
    try:
        helpful_count = int(
            driver.find_element('css selector', 'div.win.up_count').text)
    except ValueError:
        helpful_count = 0
    review_info = {'download_date': datetime.now(),
                   'source': 'https://ru.otzyv.com/',
                   'category': 'отзыв',
                   'product': driver.title,
                   'url': url,
                   'date': date,
                   'product_name': title,
                   'author': author,
                   'text': text,
                   'response': np.nan,
                   'response_date': np.nan,
                   'rating': rating,
                   'helpful_count': helpful_count,
                   'address': np.nan}
    return review_info


def parse_ru_otzyv_com(start_link: str, num_pages: int,
                       search_years: Set[int]) -> pd.DataFrame:
    """
    Collects reviews from ru.otzyv.com and saves them to a file.

    Args:
        start_link: page with reviews
        num_pages: number of pages to parse
        search_years: set of years for which the search is carried out

    Returns:
        pd.DataFrame with reviews
    """
    if not os.path.exists('res/ruotzyv'):
        os.makedirs('res/ruotzyv')

    collect_ru_otzyv_links(start_link, num_pages)

    # считываем ссылки
    with open('res/ruotzyv/ruotzyv_links.txt', 'r', encoding='utf-8') as f_in:
        links_to_parse = list_unique(
            [link.strip() for link in f_in.readlines()])

    # создаем файл с заголовками и обновляем список ссылок
    out_file_info, new_links_to_parse = create_output_file(
        'res/ruotzyv/ruotzyv_reviews.tsv', links_to_parse)
    print(f'Found {len(new_links_to_parse)} links to parse.')

    # парсим ссылки и записываем информацию в файл
    with open(out_file_info['name'], 'a', encoding='utf-8', newline='') as file:
        writer = csv.DictWriter(file, delimiter='\t',
                                fieldnames=out_file_info['fields'])
        for link in tqdm(new_links_to_parse):
            review = parse_one_ru_otzyv_page(link)
            if review:
                writer.writerow(review)

    # формируем и сохраняем датафрейм
    ruotzyv_df = pd.read_csv('res/ruotzyv/ruotzyv_reviews.tsv', sep='\t')
    ruotzyv_df = format_and_save_dataframe(ruotzyv_df, search_years)

    return ruotzyv_df


def collect_tinkoff_reviews(url: str) -> List[Dict[str, Any]]:
    """
    Collects information about tinkoff.ru reviews.

    Args:
        url: page with reviews

    Returns:
        list of dicts with review information
    """
    driver.get(url)
    while True:
        scroll_down_simple()
        try:
            driver.find_element('css selector', 'span.dBAPsP').click()
            sleep(1)
            scroll_up(3)
        except NoSuchElementException:
            break
    print('Loaded all reviews!')
    all_reviews = []
    for review in driver.find_elements('css selector', 'div.aB7zQl'):
        date = review.find_element('css selector', 'span.dlyW3E').text
        author = review.find_element('css selector', 'div.blyW3E').text
        text = review.find_element('css selector', 'span.cFci47').text
        rating = len(review.find_elements('css selector', 'svg.cXqgPj.dXqgPj'))
        helpful_count = int(
            review.find_elements(
                'css selector', 'div.a3I0Ze.b3I0Ze.g3I0Ze.n3I0Ze'
            )[2].find_element('css selector', 'span.a6P--KD').text)
        all_reviews.append({'download_date': datetime.now(),
                            'source': 'https://www.tinkoff.ru/',
                            'category': 'отзыв',
                            'product': driver.find_element(
                                'css selector', 'h1.cMdukX').text,
                            'url': url,
                            'date': date,
                            'product_name': np.nan,
                            'author': author,
                            'text': text,
                            'response': np.nan,
                            'response_date': np.nan,
                            'rating': rating,
                            'helpful_count': helpful_count,
                            'address': np.nan})
    return all_reviews


def parse_tinkoff_ru(url: str, search_years: Set[int]) -> pd.DataFrame:
    """
    Collects reviews from tinkoff.ru and saves them to a file.

    Args:
        url: page with reviews
        search_years: set of years for which the search is carried out

    Returns:
        pd.DataFrame with reviews
    """
    all_reviews = collect_tinkoff_reviews(url)
    tinkoff_df = pd.DataFrame(all_reviews)
    tinkoff_df = format_and_save_dataframe(tinkoff_df, search_years)
    return tinkoff_df


def collect_wildberries_links(link_to_products: str):
    """
    Collects links to rated wildberries.ru products.

    Args:
        link_to_products: url to the page with products

    Returns:
        None (outputs links to a file)
    """
    driver.get(link_to_products)
    scroll_down(300)
    for product in driver.find_elements('css selector',
                                        'div.product-card__wrapper'):
        rating = product.find_element(
            'css selector', 'span.address-rate-mini.address-rate-mini--sm'
        ).text.strip()
        if not rating:
            break
        prod_link = product.find_element(
            'css selector',
            'a.product-card__link.j-card-url.j-open-full-product-card'
        ).get_attribute('href')
        with open('res/wildberries/wildberries_links.txt', 'a', encoding='utf-8'
                  ) as f_out:
            f_out.write(prod_link + '\n')


def collect_wildberries_reviews(one_link: str) -> List[Dict[str, Any]]:
    """
    Collects reviews to a product.

    Args:
        one_link: product page

    Returns:
        list of dicts with review information
    """
    all_reviews = []
    print(f'Parsing {one_link}')
    driver.get(one_link)
    rev_button = WebDriverWait(driver, 10).until(
        ec.presence_of_element_located((
            'css selector', 'span.product-review__count-review.j-wba-card-' +
                            'item-show.j-wba-card-item-observe')))
    rev_button.click()
    WebDriverWait(driver, 10).until(ec.presence_of_element_located((
        'css selector', 'button.product-feedbacks__title')))
    buttons = driver.find_elements(
        'css selector', 'button.product-feedbacks__title')
    if len(buttons) > 1:
        rev_button = buttons[1]
        if 'Этот вариант товара' in rev_button.text:
            try:
                rev_button.click()
            except (ElementNotInteractableException,
                    ElementClickInterceptedException):
                pass
    WebDriverWait(driver, 10).until(ec.presence_of_element_located((
        'css selector', 'span.product-feedbacks__count')))
    rev_num = int(driver.find_elements(
        'css selector', 'span.product-feedbacks__count')[-1].text.replace(
        ' ', ''))
    if rev_num == 0:
        return all_reviews
    if rev_num > 25:
        input('Scroll down manually, then press ENTER')
    product_name = driver.find_element(
        'css selector', 'a.product-line__name.hide-mobile').text
    for review in driver.find_elements(
            'css selector',
            'li.comments__item.feedback.product-feedbacks__block-wrapper'):
        date = review.find_element('css selector', 'div.feedback__date'
                                   ).text.replace(' • Дополнен', '')
        author = review.find_element('css selector', 'p.feedback__header').text
        rating = review.find_element(
            'css selector', 'span.feedback__rating').get_attribute('class')
        rating = int(re.search(r'star(\d+)', rating).group(1))
        try:
            more_btn = review.find_element(
                'css selector', 'div.feedback__expand')
            driver.execute_script(
                "arguments[0].scrollIntoView(true);", more_btn)
            scroll_up(3)
            more_btn.click()
            sleep(1)
            text = review.find_element('css selector', 'p.feedback__text').text
        except (NoSuchElementException, ElementNotInteractableException):
            try:
                text = review.find_element(
                    'css selector', 'p.feedback__text').text
            except NoSuchElementException:
                text = np.nan
        response = np.nan
        try:
            reply = review.find_element(
                'css selector', 'span.feedback__expand.lowercase')
            driver.execute_script(
                "arguments[0].scrollIntoView(true);", reply)
            scroll_up(3)
            reply.click()
            sleep(1)
            response = review.find_element(
                'css selector',
                'p.feedback__text.feedback__text--sellers-reply.show').text
        except NoSuchElementException:
            pass
        all_reviews.append({'download_date': datetime.now(),
                            'source': 'https://www.wildberries.ru/',
                            'category': 'отзыв',
                            'product': product_name,
                            'url': one_link,
                            'date': date,
                            'product_name': np.nan,
                            'author': author,
                            'text': text,
                            'response': response,
                            'response_date': np.nan,
                            'rating': rating,
                            'helpful_count': np.nan,
                            'address': np.nan})
    return all_reviews


def parse_wildberries_ru(
        start_link: str, search_years: Set[int]) -> pd.DataFrame:
    """
    Collects reviews from wildberries.ru and saves them to a file.

    Args:
        start_link: page with products
        search_years: set of years for which the search is carried out

    Returns:
        pd.DataFrame with reviews
    """
    if not os.path.exists('res/wildberries'):
        os.makedirs('res/wildberries')

    collect_wildberries_links(start_link)

    # считываем ссылки
    with open('res/wildberries/wildberries_links.txt', 'r', encoding='utf-8'
              ) as f_in:
        links_to_parse = list_unique(
            [link.strip() for link in f_in.readlines()])

    # создаем файл с заголовками и обновляем список ссылок
    out_file_info, new_links_to_parse = create_output_file(
        'res/wildberries/wildberries_reviews.tsv', links_to_parse)
    print(f'Found {len(new_links_to_parse)} links to parse.')

    # парсим ссылки и записываем информацию в файл
    for link in tqdm(new_links_to_parse):
        with open(out_file_info['name'], 'a', encoding='utf-8', newline=''
                  ) as file:
            writer = csv.DictWriter(file, delimiter='\t',
                                    fieldnames=out_file_info['fields'])
            for review in collect_wildberries_reviews(link):
                writer.writerow(review)

    # формируем и сохраняем датафрейм
    wildberries_df = pd.read_csv(
        'res/wildberries/wildberries_reviews.tsv', sep='\t')
    wildberries_df = format_and_save_dataframe(wildberries_df, search_years)
    return wildberries_df


def collect_crmindex_reviews(url: str) -> List[Dict[str, Any]]:
    """
    Collects reviews from crmindex.ru.

    Args:
        url: page with reviews

    Returns:
        list of dicts with review information
    """
    driver.get(url)
    reviews = []
    product_name = WebDriverWait(driver, 10).until(
        ec.presence_of_element_located((
            'css selector', 'h1.h3.no-translate-weglot'))).text.strip()
    for review in driver.find_elements(
            'css selector', 'div.catalog-item-review-item'):
        driver.execute_script("arguments[0].scrollIntoView(true);", review)
        sleep(1)
        date = review.find_element(
            'css selector', 'div.review-date-name').find_element(
            'tag name', 'p').text
        title = review.find_element('css selector', 'div.product_name').text
        author = review.find_element('css selector', 'div.social-name').text
        text = review.find_element('css selector', 'div.review-generally').text
        rating = len(review.find_elements(
            'css selector', 'i.fa.fa-star.yellow'))

        reviews.append({'download_date': datetime.now(),
                        'source': 'https://crmindex.ru/',
                        'category': 'отзыв',
                        'product': product_name,
                        'url': url,
                        'date': date,
                        'product_name': title,
                        'author': author,
                        'text': text,
                        'response': np.nan,
                        'response_date': np.nan,
                        'rating': rating,
                        'helpful_count': np.nan,
                        'address': np.nan})
    return reviews


def parse_crmindex_ru(url: str, search_years: Set[int]) -> pd.DataFrame:
    """
    Collects reviews from crmindex.ru and saves them to a file.

    Args:
        url: page with reviews
        search_years: set of years for which the search is carried out

    Returns:
        pd.DataFrame with reviews
    """
    all_reviews = collect_crmindex_reviews(url)
    crmindex_df = pd.DataFrame(all_reviews)
    crmindex_df = format_and_save_dataframe(crmindex_df, search_years)
    return crmindex_df


def parse_yandex_reviews_page(url: str, search_years: Set[int]):
    """
    Collect reviews information from one reviews.yandex.ru page.

    Args:
        url: webpage with reviews
        search_years: set of years for which the search is carried out

    Returns:
        list of dicts with review information
    """
    driver.get(url)
    btn = WebDriverWait(driver, 5).until(
        ec.presence_of_element_located((
            'css selector',
            'button.ReviewPageInfoTile.ReviewsPageMainInfoTilesList-' +
            'Item.ReviewsPageMainInfoTilesList-ReviewsTile')))
    btn.click()
    btn = WebDriverWait(driver, 5).until(
        ec.presence_of_element_located((
            'css selector',
            'button.Button.Button_width_max.Button_size_m.' +
            'Button_view_default.Select2-Button')))
    btn.click()
    WebDriverWait(driver, 5).until(
        ec.presence_of_element_located((
            'css selector', 'label.Radiobox-Radio')))
    sel = driver.find_elements('css selector', 'label.Radiobox-Radio')[1]
    sel.click()
    product_name = driver.find_element(
        'css selector', 'h2.ReviewsPageMainTitle-Title').text
    reviews = []
    while True:
        scroll_down_simple()
        try:
            more = driver.find_element(
                'css selector',
                'button.Button.Button_width_max.Button_view_default.' +
                'Button_size_m.ReviewMoreButton-Button')
            more.click()
            sleep(3)
        except NoSuchElementException:
            break
        last_date = driver.find_elements(
            'css selector', 'a.Link.Review-Date')[-1].text
        if not last_date.strip()[-1].isalpha() and not any(
                str(year) in last_date for year in search_years):
            break
    rev_elts = driver.find_elements('css selector', 'li.Review')
    print(f'Found {len(rev_elts)} reviews in the search_years range!')
    for review in rev_elts:
        driver.execute_script("arguments[0].scrollIntoView(true);", review)
        try:
            review.find_element(
                'css selector', 'span.Link.Link_theme_ghost').click()
            sleep(1)
        except NoSuchElementException:
            pass
        text = review.find_element('css selector', 'div.Review-Text').text
        try:
            date = review.find_element(
                'css selector', 'a.Link.Review-Date').text
        except NoSuchElementException:
            date = review.find_element(
                'css selector', 'span.Review-Date').text
        author = review.find_element(
            'css selector', 'span.ReviewAuthor-Name').text
        rating = review.find_element(
            'css selector', 'div.Review-Rating').get_attribute('aria-label')
        rating = int(re.search(r'оценка (\d) из 5', rating).group(1))
        reviews.append({'download_date': datetime.now(),
                        'source': 'https://reviews.yandex.ru/',
                        'category': 'отзыв',
                        'product': product_name,
                        'url': url,
                        'date': date,
                        'product_name': np.nan,
                        'author': author,
                        'text': text,
                        'response': np.nan,
                        'response_date': np.nan,
                        'rating': rating,
                        'helpful_count': np.nan,
                        'address': np.nan})
    return reviews


def parse_yandex_reviews(url: str, search_years: Set[int]) -> pd.DataFrame:
    """
    Collects reviews from reviews.yandex.ru and saves them to a file.

    Args:
        url: page with reviews
        search_years: set of years for which the search is carried out

    Returns:
        pd.DataFrame with reviews
    """
    all_reviews = parse_yandex_reviews_page(url, search_years)
    yanrev_df = pd.DataFrame(all_reviews)
    yanrev_df = format_and_save_dataframe(yanrev_df, search_years)
    return yanrev_df


def collect_yandex_market_links(url: str):
    """
    Collects links to products from one market.yandex.ru page.

    Args:
        url: page with products

    Returns:
        None (saves the information to a file)
    """
    driver.get(url)
    try:
        WebDriverWait(driver, 5).until(
            ec.presence_of_element_located((
                'css selector', 'button._3UJ2B.cia-cs'))).click()
        sleep(3)
    except TimeoutException:
        pass
    start_product_num = len(driver.find_elements(
        'css selector', 'div._2rw4E.D81hX._2g7lE'))
    while True:
        scroll_down(250)
        products = driver.find_elements(
            'css selector', 'div._2rw4E.D81hX._2g7lE')
        last_product = products[-1]
        # если дошли до продуктов без отзывов, останавливаемся
        try:
            last_product.find_element('css selector', 'span.ds-rating')
        except NoSuchElementException:
            break
        # если продукты больше не подгружаются, останавливаемся
        if len(products) > start_product_num:
            start_product_num = len(products)
        else:
            break
    products = driver.find_elements('css selector', 'div._2rw4E.D81hX._2g7lE')
    print(f'Scrolling complete, found {len(products)} products!')
    for product in products:
        try:
            product.find_element('css selector', 'span.ds-rating')
            prod_link = product.find_element(
                'css selector', 'a.EQlfk').get_attribute('href')
            with open('res/yandex_market/yandex_market_links.txt', 'a',
                      encoding='utf-8') as f_out:
                f_out.write(prod_link + '\n')
        except NoSuchElementException:
            break


def collect_yandex_market_reviews(
        url: str, search_years: Set[int]) -> List[Dict[str, Any]]:
    """
    Collects reviews for one product.

    Args:
        url: page with reviews
        search_years: set of years for which the search is carried out

    Returns:
        list of dicts with review information
    """
    driver.get(url)
    all_reviews = []
    # переходим к отзывам
    try:
        WebDriverWait(driver, 5).until(
            ec.presence_of_element_located((
                'css selector',
                'a.EQlfk.ds-flex_inl.ds-flex_ai_c.eQb-4'))).click()
    except TimeoutException:
        try:
            sold_out = driver.find_element(
                'css selector',
                'h2.ds-text.ds-text_weight_bold.ds-text_typography_headline-5' +
                '.ds-text_headline-5_tight.ds-text_headline-5_bold').text
            if 'Нет в продаже' in sold_out:
                return all_reviews
        except NoSuchElementException:
            no_seller = driver.find_element(
                'css selector',
                'div.div.ds-text.ds-text_weight_reg.ds-text_typography_lead-' +
                'text._2JkzZ.ds-text_lead-text_loose.ds-text_lead-text_reg')
            if 'У этого продавца нет отзывов о товаре' in no_seller:
                return all_reviews
    try:
        WebDriverWait(driver, 10).until(
            ec.presence_of_element_located((
                'css selector', 'button.ds-segmented-control__segment-item')))
        # выбираем конкретный вариант товара
        driver.find_elements(
            'css selector', 'button.ds-segmented-control__segment-item')[
            1].click()
    except TimeoutException:
        pass
    # находим и закрываем попап
    WebDriverWait(driver, 10).until(
        ec.presence_of_element_located((
            'css selector', 'div._3Z1xW.Trrr0._1MOwX._1bCJz')))
    try:
        ActionChains(driver).move_by_offset(10, 10).click().perform()
    except MoveTargetOutOfBoundsException:
        input('Please close the popup manually, then press ENTER')
    # раскрываем меню сортировки
    WebDriverWait(driver, 5).until(
        ec.presence_of_element_located((
            'css selector', 'button._3ooNj.cn8EZ._21ijP'))).click()
    WebDriverWait(driver, 5).until(
        ec.presence_of_element_located((
            'css selector',
            'label.ds-radio.ds-radio_size_xs.ds-flex.ds-flex_row.py7R7')))
    # выбираем сортировку по новизне
    driver.find_elements(
        'css selector',
        'label.ds-radio.ds-radio_size_xs.ds-flex.ds-flex_row.py7R7')[-1].click()
    sleep(1)
    start_review_num = len(driver.find_elements(
            'css selector', 'div[data-apiary-widget-name="@card/ReviewItem"]'))
    while True:
        scroll_down(50)
        reviews = driver.find_elements(
            'css selector', 'div[data-apiary-widget-name="@card/ReviewItem"]')
        last_date = reviews[-1].find_element(
            'css selector', 'div.ds-trainLine').text
        # если последний отзыв слишком старый, останавливаемся
        if not last_date.strip()[-1].isalpha() and not any(
                str(year) in last_date for year in search_years):
            break
        # если продукты больше не подгружаются, останавливаемся
        if len(reviews) > start_review_num:
            start_review_num = len(reviews)
        else:
            break

    product_name = driver.find_element('css selector', 'h1.ds-text').text
    for review in driver.find_elements(
            'css selector', 'div[data-apiary-widget-name="@card/ReviewItem"]'):
        driver.execute_script("arguments[0].scrollIntoView(true);", review)
        date = review.find_element('css selector', 'div.ds-trainLine').text
        author = review.find_element(
            'css selector', 'span[itemprop="name"]').text
        try:
            review.find_element(
                'xpath', './/button[contains(text(), "...ещё")]').click()
            sleep(1)
        except NoSuchElementException:
            pass
        text = review.find_element('css selector', 'div._1xJG9').text
        rating = review.find_element(
            'css selector', 'div.lww0t').get_attribute('style')
        rating = int(re.search(r'width: (\d+)%', rating).group(1)) / 20
        helpful_count = int(review.find_elements(
            'css selector',
            'span.ds-text.ds-text_lineClamp_1.ds-text_weight_med.ds' +
            '-text_color_text-secondary.ds-text_proportional.ds' +
            '-text_typography_text.ds-text_text_tight.ds-text_text_med.ds' +
            '-text_lineClamp')[2].text)
        all_reviews.append({'download_date': datetime.now(),
                            'source': 'https://market.yandex.ru/',
                            'category': 'отзыв',
                            'product': product_name,
                            'url': url,
                            'date': date,
                            'product_name': np.nan,
                            'author': author,
                            'text': text,
                            'response': np.nan,
                            'response_date': np.nan,
                            'rating': rating,
                            'helpful_count': helpful_count,
                            'address': np.nan})

    return all_reviews


def parse_yandex_market(
        start_link: str, search_years: Set[int]) -> pd.DataFrame:
    """
    Collects reviews from market.yandex.ru and saves them to a file.

    Args:
        start_link: page with products
        search_years: set of years for which the search is carried out

    Returns:
        pd.DataFrame with reviews
    """
    if not os.path.exists('res/yandex_market'):
        os.makedirs('res/yandex_market')

    collect_yandex_market_links(start_link)

    # считываем ссылки
    with open('res/yandex_market/yandex_market_links.txt', 'r', encoding='utf-8'
              ) as f_in:
        links_to_parse = list_unique(
            [link.strip() for link in f_in.readlines()])

    # создаем файл с заголовками и обновляем список ссылок
    out_file_info, new_links_to_parse = create_output_file(
        'res/yandex_market/yandex_market_reviews.tsv', links_to_parse)
    if os.path.exists('res/yandex_market/empty_links.txt'):
        with open('res/yandex_market/empty_links.txt', 'r',
                  encoding='utf-8') as f_bad:
            bad_links = list_unique(f_bad.read().split('\n'))
        new_links_to_parse = [link.strip() for link in new_links_to_parse
                              if link not in bad_links]
    print(f'Found {len(new_links_to_parse)} links to parse.')

    # парсим ссылки и записываем информацию в файл
    for link in tqdm(new_links_to_parse):
        print(link)
        with open(out_file_info['name'], 'a', encoding='utf-8', newline=''
                  ) as file:
            writer = csv.DictWriter(file, delimiter='\t',
                                    fieldnames=out_file_info['fields'])
            reviews = collect_yandex_market_reviews(link, search_years)
            for review in reviews:
                writer.writerow(review)
            if not reviews:
                with open('res/yandex_market/empty_links.txt', 'a',
                          encoding='utf-8') as f_bad:
                    f_bad.write(link + '\n')
        sleep(5)

    # формируем и сохраняем датафрейм
    yamarket_df = pd.read_csv(
        'res/yandex_market/yandex_market_reviews.tsv', sep='\t')
    yamarket_df = format_and_save_dataframe(yamarket_df, search_years)
    return yamarket_df


def collect_dns_categories(start_link: str):
    """
    Collects links to different product categories.

    Args:
        start_link: page with product categories

    Returns:
        None (saves the output to a file)
    """
    driver.get(start_link)
    link_cont = WebDriverWait(driver, 10).until(
        ec.presence_of_element_located((
            'css selector',
            'div._categories__wrapper_te7j_27')))
    main_links = [elt.get_attribute('href') for elt in
                  link_cont.find_elements('tag name', 'a')]
    WebDriverWait(driver, 5).until(
        ec.presence_of_element_located((
            'css selector',
            'span._categories-show-more__button-title_zu9pz_193'))).click()
    sleep(1)
    add_links = [
        elt.get_attribute('href') for elt in driver.find_elements(
            'css selector', 'a._menu__category_link_t05lm_12')]
    with open('res/dns/dns_categories.txt', 'a', encoding='utf-8'
              ) as f_out:
        for link in list_unique(main_links + add_links):
            f_out.write(link + '\n')


def collect_dns_products(url: str):
    """
    Collects links to products in one category.

    Args:
        url: page with products

    Returns:
        None (saves the output to a file)
    """
    driver.get(f'{url}&order=4')  # сортировка по кол-ву отзывов
    scroll_down_simple()
    with open('res/dns/dns_links.txt', 'a', encoding='utf-8') as f_out:
        for product in driver.find_elements(
                'css selector', 'div.catalog-product'):
            driver.execute_script("arguments[0].scrollIntoView(true);", product)
            if 'нет отзывов' in product.find_element(
                    'css selector', 'a.catalog-product__rating').text:
                break
            link = product.find_element(
                'css selector', 'a.catalog-product__name').get_attribute('href')
            f_out.write(link + '\n')
    sleep(5)


def collect_dns_reviews(
        url: str, search_years: Set[int]) -> List[Dict[str, Any]]:
    """
    Collects reviews for one product.

    Args:
        url: page with product reviews
        search_years: set of years for which the search is carried out

    Returns:
        list of dicts with review information
    """
    driver.get(url)
    WebDriverWait(driver, 5).until(
        ec.presence_of_element_located((
            'css selector', 'a.product-card-top__rating'))).click()
    scroll_down(100)
    product_name = driver.find_element(
        'css selector', 'h1.product-card-top__title').text
    try:
        WebDriverWait(driver, 10).until(
            ec.presence_of_element_located((
                'css selector', 'div.ow-filters__count-filter-btn')))
        load_btn = driver.find_elements(
            'css selector', 'div.ow-filters__count-filter-btn')[1]
        driver.execute_script("arguments[0].scrollIntoView(true);", load_btn)
        scroll_up(5)
        load_btn.click()
        sleep(3)
    except TimeoutException:
        pass
    reviews = driver.find_elements('css selector', 'div.ow-opinions__item')
    start_review_num = len(reviews)
    while True:
        # если последний отзыв слишком старый, останавливаемся
        if not any(str(year) in reviews[-1].find_element(
                   'css selector', 'span.ow-opinion__date').text
                   for year in search_years):
            break
        try:
            more_btn = WebDriverWait(driver, 5).until(
                ec.presence_of_element_located((
                    'css selector', 'button.paginator-widget__more')))
            driver.execute_script(
                "arguments[0].scrollIntoView(true);", more_btn)
            scroll_up(5)
            more_btn.click()
            sleep(5)
            reviews = driver.find_elements(
                'css selector', 'div.ow-opinions__item')
            # если продукты больше не подгружаются, останавливаемся
            if len(reviews) > start_review_num:
                start_review_num = len(reviews)
            else:
                break
        except (TimeoutException, ElementNotInteractableException):
            break

    all_reviews = []
    for review in driver.find_elements('css selector', 'div.ow-opinions__item'):
        driver.execute_script("arguments[0].scrollIntoView(true);", review)
        date = review.find_element('css selector', 'span.ow-opinion__date').text
        author = review.find_element(
            'css selector', 'div.profile-info__name').text
        text = review.find_element('css selector', 'div.ow-opinion__texts').text
        rating = [star.get_attribute('data-state') for star in
                  review.find_elements(
                      'css selector', 'div.star-rating__star')
                  ].count('selected') / 2
        try:
            helpful_count = int(review.find_element(
                'css selector', 'span.ow-vote__sum').text)
        except NoSuchElementException:
            # дальше идут пустые отзывы (оценка без текста)
            break
        response, response_date = np.nan, np.nan
        comments = review.find_element(
            'css selector', 'div.ow-opinion__comments')
        if 'Комментировать' not in comments.text:
            driver.execute_script(
                "arguments[0].scrollIntoView(true);", comments)
            scroll_up(5)
            comments.find_element('css selector', 'a.ui-collapse__link').click()
            sleep(1)
            for comment in review.find_elements('css selector', 'div.comment'):
                if 'SberDevices' in comment.find_element(
                        'css selector', 'div.profile-info__name').text:
                    response = comment.find_element(
                        'css selector', 'div.comment__message').text
                    response_date = comment.find_element(
                        'css selector', 'span.comment__date.time-info').text
        all_reviews.append({'download_date': datetime.now(),
                            'source': 'https://www.dns-shop.ru/',
                            'category': 'отзыв',
                            'product': product_name,
                            'url': url,
                            'date': date,
                            'product_name': np.nan,
                            'author': author,
                            'text': text,
                            'response': response,
                            'response_date': response_date,
                            'rating': rating,
                            'helpful_count': helpful_count,
                            'address': np.nan})
    return all_reviews


def parse_dns(start_link: str, search_years: Set[int]) -> pd.DataFrame:
    """
    Collects reviews for products available at dns-shop.ru.

    Args:
        start_link: page with product categories
        search_years: set of years for which the search is carried out

    Returns:
        pd.DataFrame with reviews
    """
    if not os.path.exists('res/dns'):
        os.makedirs('res/dns')

    # собираем ссылки на категории продуктов
    collect_dns_categories(start_link)

    # считываем ссылки на категории продуктов
    with open('res/dns/dns_categories.txt', 'r', encoding='utf-8') as f_in:
        links_to_cats = list_unique([link.strip() for link in f_in.readlines()])
        print(f'Found {len(links_to_cats)} product categories.')

    # собираем ссылки на продукты, имеющие отзывы
    for link in tqdm(links_to_cats):
        collect_dns_products(link)

    # считываем ссылки на продукты
    with open('res/dns/dns_links.txt', 'r', encoding='utf-8') as f_in:
        links_to_parse = list_unique(
            [link.strip() for link in f_in.readlines()])

    # создаем файл с заголовками и обновляем список ссылок
    out_file_info, new_links_to_parse = create_output_file(
        'res/dns/dns_reviews.tsv', links_to_parse)
    print(f'Found {len(new_links_to_parse)} links to parse.')

    # парсим ссылки и записываем информацию в файл
    for link in tqdm(new_links_to_parse):
        with open(out_file_info['name'], 'a', encoding='utf-8', newline=''
                  ) as file:
            writer = csv.DictWriter(file, delimiter='\t',
                                    fieldnames=out_file_info['fields'])
            for review in collect_dns_reviews(link, search_years):
                writer.writerow(review)

    # формируем и сохраняем датафрейм
    dns_df = pd.read_csv('res/dns/dns_reviews.tsv', sep='\t')
    dns_df = format_and_save_dataframe(dns_df, search_years)
    return dns_df


def collect_eldorado_links(start_link: str):
    """
    Collects links to rated products.

    Args:
        start_link: page with products

    Returns:
        None (saves the output to a file)
    """
    driver.get(start_link)
    while True:
        scroll_down(200)
        # если товаров с отзывами больше нет, останавливаемся
        if driver.find_elements(
                'css selector', 'a.elEO')[-1].text.strip() == '0 отзывов':
            break
        try:
            more_btn = WebDriverWait(driver, 5).until(
                ec.presence_of_element_located((
                    'css selector', 'button[data-dy="show-more"]')))
            driver.execute_script(
                "arguments[0].scrollIntoView(true);", more_btn)
            scroll_up(5)
            more_btn.click()
        except TimeoutException:
            break
    products = driver.find_elements('css selector', 'li.elsO.sl-hl-checked')
    print(f'Found no more than {len(products)} rated products.')
    with open('res/eldorado/eldorado_links.txt', 'a',
              encoding='utf-8') as f_out:
        for product in products:
            if product.find_element(
                    'css selector', 'a.elEO').text.strip() == '0 отзывов':
                break
            prod_link = product.find_element(
                'css selector', 'a.elBO').get_attribute('href')
            f_out.write(prod_link + '\n')


def collect_eldorado_reviews(url: str, search_years: Set[int],
                             captcha: bool = False) -> List[Dict[str, Any]]:
    """
    Collects reviews for one prduct.

    Args:
        url: page with product reviews
        search_years: set of years for which the search is carried out
        captcha: whether to request the user to manually pass captcha

    Returns:
        list of dicts with review information
    """
    driver.get(f'{url}?show=response#TabReviews')
    if captcha:
        input('Press ENTER after filling CAPTCHA')
    product_name = WebDriverWait(driver, 5).until(
        ec.presence_of_element_located(
            ('css selector', 'h2.elni.elqi.eleE'))).text
    # открываем раздел с отзывами
    try:
        rev_btn = WebDriverWait(driver, 5).until(ec.presence_of_element_located(
            ('css selector', 'button.elSA.elTA.eleQ')))
        driver.execute_script("arguments[0].scrollIntoView(true);", rev_btn)
        scroll_up(5)
        rev_btn.click()
        sleep(2)
        # сортировка по новизне
        driver.find_elements('css selector', 'button.elPE.elRE')[2].click()
        sleep(0.5)
        driver.find_elements('css selector', 'span.elxF')[1].click()
        sleep(2)
        while True:
            scroll_down_simple()
            last_review_date = driver.find_elements(
                'css selector', 'div.elQU')[-1].find_element(
                'css selector', 'span.elXU').text
            # если последний отзыв слишком старый, останавливаемся
            if not any(str(year) in last_review_date for year in search_years):
                break
            # если нет кнопки "показать еще", останавливаемся
            try:
                rev_btn = WebDriverWait(driver, 5).until(
                    ec.presence_of_element_located(
                        ('css selector', 'button.eljx.elpx.elrx.elmx')))
                driver.execute_script(
                    "arguments[0].scrollIntoView(true);", rev_btn)
                rev_btn.click()
            except TimeoutException:
                break
    except TimeoutException:
        pass
    all_reviews = []
    for review in driver.find_elements('css selector', 'div.elQU'):
        driver.execute_script("arguments[0].scrollIntoView(true);", review)
        text = review.find_element('css selector', 'p.elYU').text
        if not text:
            continue
        date = review.find_element('css selector', 'span.elXU').text
        author = review.find_element('css selector', 'span.elUU').text
        rating = float(len(re.findall(r'c1826360',
                                      review.get_attribute('innerHTML'))))
        helpful_count = int(review.find_element(
            'css selector', 'span.elBW').text)
        all_reviews.append({'download_date': datetime.today(),
                            'source': 'https://www.eldorado.ru/',
                            'category': 'отзыв',
                            'product': product_name,
                            'url': url,
                            'date': date,
                            'product_name': np.nan,
                            'author': author,
                            'text': text,
                            'response': np.nan,
                            'response_date': np.nan,
                            'rating': rating,
                            'helpful_count': helpful_count,
                            'address': np.nan})
    return all_reviews


def parse_eldorado(start_link: str, search_years: Set[int]) -> pd.DataFrame:
    """
    Collects reviews for products available at eldorado.ru.

    Args:
        start_link: page with products
        search_years: set of years for which the search is carried out

    Returns:
        pd.DataFrame with reviews
    """
    if not os.path.exists('res/eldorado'):
        os.makedirs('res/eldorado')

    # собираем ссылки на продукты
    collect_eldorado_links(start_link)

    # считываем ссылки на продукты
    with open('res/eldorado/eldorado_links.txt', 'r', encoding='utf-8') as f_in:
        links_to_parse = list_unique(
            [link.strip() for link in f_in.readlines()])

    # создаем файл с заголовками и обновляем список ссылок
    out_file_info, new_links_to_parse = create_output_file(
        'res/eldorado/eldorado_reviews.tsv', links_to_parse)
    print(f'Found {len(new_links_to_parse)} links to parse.')

    # парсим ссылки и записываем информацию в файл
    for i, link in tqdm(enumerate(new_links_to_parse),
                        total=len(new_links_to_parse)):
        with open(out_file_info['name'], 'a', encoding='utf-8', newline=''
                  ) as file:
            writer = csv.DictWriter(file, delimiter='\t',
                                    fieldnames=out_file_info['fields'])
            if i == 0:
                reviews = collect_eldorado_reviews(link, search_years,
                                                   captcha=True)
            else:
                reviews = collect_eldorado_reviews(link, search_years)
            for review in reviews:
                writer.writerow(review)

    # формируем и сохраняем датафрейм
    eldorado_df = pd.read_csv('res/eldorado/eldorado_reviews.tsv', sep='\t')
    eldorado_df = format_and_save_dataframe(eldorado_df, search_years)
    return eldorado_df


def get_mvideo_product_links(category_links: List[str]) -> List[str]:
    """
    Iterates over product categories and collects links to products.

    Args:
        category_links: links to product categories

    Returns:
        product links
    """
    page_links = []
    for url in category_links:
        driver.get(url)
        products_chunks = driver.find_elements(
            'css selector',
            'h3.fl-product-tile-product_name.fl-product-tile__title')
        for product_link in products_chunks:
            product_link_short = re.search(
                r'href="/(.*?)"',
                product_link.get_attribute('innerHTML')).group(1)
            link = f'https://www.mvideo.ru/{product_link_short}/reviews'
            page_links.append(link)
    return page_links


def get_product_reviews_mvideo(url: str) -> List[Dict[str, Any]]:
    """
    Collects reviews for one product.

    Args:
        url: url to product page

    Returns:
        list of dicts with review information
    """
    driver.get(url)
    sleep(1)
    while True:
        scroll_down_simple()
        sleep(2)
        try:
            button = WebDriverWait(driver, 5).until(
                ec.presence_of_element_located(
                    ('css selector', 'button.show-all')))
            driver.execute_script("arguments[0].scrollIntoView(true);", button)
            scroll_up(5)
            sleep(0.5)
            button.click()
        except (ElementClickInterceptedException,
                ElementNotInteractableException,
                StaleElementReferenceException,
                TimeoutException):
            break

    reviews = []
    for review in tqdm(driver.find_elements(
            'tag name', 'mvid-extended-review')):
        author = review.find_element('css selector', 'span.head__name').text
        date = review.find_element('css selector', 'span.head__date').text
        title = review.find_element('tag name', 'meta').get_attribute('content')
        rating = float(
            review.find_element('css selector', 'span.rating-value.small').text)
        text = review.find_element('css selector', 'div.review-text').text
        sleep(1)
        official_response, official_response_date = np.nan, np.nan
        response_container = review.find_elements(
            'css selector', 'div.comment.ng-star-inserted')
        if len(response_container) != 0:
            for response in response_container:
                if response.find_element(
                        'css selector'
                        'span.product_name').text == 'SberDevices':
                    official_response = response.find_element(
                        'css selector',
                        'mvid-abbreviated-text.comment__text').text
                    official_response_date = response.find_element(
                        'css selector', 'span.head__date').text
        helpful_count = review.find_element('css selector', 'div.like').text
        reviews.append(
            {'download_date': datetime.today(),
             'source': 'https://www.mvideo.ru/',
             'category': 'отзыв',
             'product': title,
             'url': url,
             'date': date,
             'author': author,
             'product_name': np.nan,
             'text': text,
             'response': official_response,
             'response_date': official_response_date,
             'rating': rating,
             'helpful_count': helpful_count,
             'address': np.nan})

    return reviews


def parse_mvideo(
        start_links: List[str], search_years: Set[int]) -> pd.DataFrame:
    """
    Collects reviews for products available at mvideo.ru.

    Args:
        start_links: pages with product categories
        search_years: set of years for which the search is carried out

    Returns:
        pd.DataFrame with reviews
    """
    if not os.path.exists('res/mvideo'):
        os.makedirs('res/mvideo')

    links_to_parse = get_mvideo_product_links(start_links)

    # создаем файл с заголовками и обновляем список ссылок
    out_file_info, new_links_to_parse = create_output_file(
        'res/mvideo/mvideo_reviews.tsv', links_to_parse)
    print(f'Found {len(new_links_to_parse)} links to parse.')

    # парсим ссылки и записываем информацию в файл
    with open(out_file_info['name'], 'a', encoding='utf-8', newline='') as file:
        writer = csv.DictWriter(file, delimiter='\t',
                                fieldnames=out_file_info['fields'])

        for link in tqdm(new_links_to_parse):
            link_reviews_list = get_product_reviews_mvideo(link)
            for review in link_reviews_list:
                writer.writerow(review)

    # формируем и сохраняем датафрейм
    mvideo_df = pd.read_csv('res/mvideo/mvideo_reviews.tsv', sep='\t')
    mvideo_df = format_and_save_dataframe(mvideo_df, search_years)
    return mvideo_df


def get_megamarket_product_links(company_name: str) -> List[str]:
    """
    Collects links to products available at megamarket.ru.

    Args:
        company_name: brand name

    Returns:
        list of product links
    """
    page_links = []
    for page_number in range(1, 11):
        driver.get(f'https://megamarket.ru/brands/{company_name}/page'
                   f'-{page_number}/#?related_search=sber')
        scroll_down_simple()
        if len(driver.find_elements(
                'css selector',
                'div.catalog-item-regular-desktop.ddl_product.catalog-item-' +
                'desktop')) != 0:
            for i in driver.find_elements(
                    'css selector',
                    'div.catalog-item-regular-desktop__main-info'):
                page_links.append(
                    'https://megamarket.ru/' + re.search(
                        r'href="/(.*?)"', i.get_attribute(
                            'innerHTML')).group(
                        1) + 'otzyvy/#?details_block=reviews')
        elif len(driver.find_elements(
            'css selector',
                'div.catalog-item-regular-desktop.ddl_product.catalog-item-' +
                'regular-desktop_out-of-stock.catalog-item-desktop')) != 0:
            for i in driver.find_elements(
                    'css selector',
                    'div.catalog-item-regular-desktop.ddl_product.catalog-' +
                    'item-regular-desktop_out-of-stock.catalog-item-desktop'):
                page_links.append(
                    'https://megamarket.ru/' + re.search(
                        r'href="/(.*?)"', i.get_attribute('innerHTML')).group(
                        1) + 'otzyvy/#?details_block=reviews')
        else:
            pass
    return page_links


def parse_one_page_megamarket(
        company_name: str, url: str, product_name: str) -> List[Dict[str, Any]]:
    """
    Collects reviews for one product from one page.

    Args:
        company_name: brand name
        url: url to reviews page
        product_name: product name

    Returns:
        list of dicts with review information
    """
    sleep(3)
    all_revs = driver.find_elements('css selector', 'div.review-item')
    reviews = []
    for review in tqdm(all_revs):
        driver.execute_script("arguments[0].scrollIntoView(true);", review)
        author = review.find_element(
            'css selector', 'span.review-item-header__author').text
        date = review.find_element(
            'css selector', 'time.review-item-header__date').text
        rating = float(len(review.find_elements(
            'css selector',
            'div.pui-rating-display__item.pui-rating-item.pui-rating-item_' +
            'variant_primary.pui-rating-item_selected')))
        text = '\n'.join([i.text for i in review.find_elements(
            'css selector', 'div.review-item__body.text-block')]).strip('\n')
        helpful_count = int(review.find_element(
            'css selector', 'div.vote.vote_like').text)
        official_response, official_response_date = np.nan, np.nan
        response_container = review.find_elements(
            'css selector', 'div.review-comment')
        if len(response_container) != 0:
            for response in response_container:
                if 'SberDevices' in response.find_element(
                        'css selector', 'h4.review-comment__title').text:
                    official_response = response.find_element(
                        'css selector', 'p.review-comment__text').text
                    official_response_date = response.find_element(
                        'css selector', 'span.review-comment__date').text
        reviews.append(
            {'download_date': datetime.today(),
             'source': company_name,
             'category': 'отзыв',
             'product': product_name,
             'url': url,
             'date': date,
             'author': author,
             'product_name': np.nan,
             'text': text,
             'response': official_response,
             'response_date': official_response_date,
             'rating': rating,
             'helpful_count': helpful_count,
             'address': np.nan})
    return reviews


def get_one_product_reviews_megamarket(
        url: str, company_name: str) -> List[Dict[str, Any]]:
    """
    Collects all reviews for one product.

    Args:
        url: url to product page
        company_name: brand name

    Returns:
        list of dicts with review information
    """
    driver.get(url)
    print(url)
    sleep(0.5)
    try:
        title = driver.find_element(
            'css selector',
            'h1.pdp-header__title.pdp-header__title_only-product_name').text
    except NoSuchElementException:
        title = driver.find_element('css selector',
                                    'h1.pdp-header__subtitle').text
    pages_count = driver.find_elements(
        'css selector',
        'button.pui-pagination-control.pui-pagination-control_size_lg')
    print('pages_count: ', len(pages_count))

    current_page = 1
    reviews = []
    while True:
        print(f'Parsing page {current_page}')

        try:
            reviews.extend(
                parse_one_page_megamarket(company_name, url, title))

            next_button = WebDriverWait(driver, 10).until(
                ec.presence_of_element_located(
                    ('css selector',
                     'div.pui-pagination__controls > ' +
                     'div.pui-button-wrapper:last-child button')))

            driver.execute_script(
                "arguments[0].scrollIntoView({block: 'center'});", next_button)

            if 'disabled' in next_button.get_attribute('class'):
                print("Reached last page")
                break

            driver.execute_script('arguments[0].click();', next_button)

            WebDriverWait(driver, 10).until(
                ec.presence_of_element_located(
                    (By.CSS_SELECTOR, 'div.review-item')))

            WebDriverWait(driver, 10).until(
                ec.text_to_be_present_in_element(
                    (By.CSS_SELECTOR,
                     'div.pui-pagination__controls button.' +
                     'pui-pagination-control_selected'),
                    str(current_page + 1)))

            current_page += 1

        except (NoSuchElementException, TimeoutException,
                StaleElementReferenceException) as e:
            print(f'No more pages or error navigating: {e}')
            with open('trouble_links.txt', 'a', encoding='utf-8') as file:
                file.write(
                    f'{e}: {url}, ' +
                    f'page number: {current_page}\n')
            break

    print(f'Total reviews collected: {len(reviews)}')
    return reviews


def parse_megamarket(company_name: str, search_years: Set[int]) -> pd.DataFrame:
    """
    Collects reviews for products available at mvideo.ru.

    Args:
        company_name: brand name
        search_years: set of years for which the search is carried out

    Returns:
        pd.DataFrame with reviews
    """
    links_to_parse = get_megamarket_product_links(company_name)

    # создаем файл с заголовками и обновляем список ссылок
    out_file_info, new_links_to_parse = create_output_file(
        'res/megamarket/megamarket_reviews.tsv', links_to_parse)
    print(f'Found {len(new_links_to_parse)} links to parse.')

    # парсим ссылки и записываем информацию в файл
    with open(out_file_info['name'], 'a', encoding='utf-8', newline='') as file:
        writer = csv.DictWriter(file, delimiter='\t',
                                fieldnames=out_file_info['fields'])

        for link in tqdm(new_links_to_parse):
            link_reviews_list = get_one_product_reviews_megamarket(
                link, company_name)
            for review in link_reviews_list:
                writer.writerow(review)

    megamarket_df = pd.read_csv(
        'res/megamarket/megamarket_reviews.tsv', sep='\t')
    megamarket_df = format_and_save_dataframe(megamarket_df, search_years)
    return megamarket_df


def form_final_dataframe(tables: List[str], output_file: str) -> pd.DataFrame:
    """
    Merges dataframes with reviews from various websites and unifies the format.

    Args:
        tables: list of files with reviews
        output_file: name of the file to which the results are saved

    Returns:
        final pd.DataFrame with all reviews
    """
    merged_df = pd.read_excel(tables[0])
    for file in tables[1:]:
        add_df = pd.read_excel(file)
        merged_df = pd.concat([merged_df, add_df])
    for column in merged_df.columns:
        if 'Unnamed' in column:
            merged_df.drop(columns=[column], inplace=True)

    merged_df.drop_duplicates(inplace=True)
    merged_df.applymap(replace_empty_strings)
    merged_df.dropna(subset=['text'], inplace=True)
    merged_df.reset_index(drop=True, inplace=True)
    merged_df = convert_dates_in_dataframe(merged_df)

    merged_df.sort_values(
        by=['source', 'product', 'url', 'date', 'product_name', 'author',
            'text', 'rating', 'helpful_count', 'download_date'],
        ascending=[True, True, True, True, True, True, True, True, True, False],
        inplace=True)
    merged_df.drop_duplicates(subset=[
        'source', 'product', 'url', 'date', 'product_name', 'author', 'text',
        'rating', 'helpful_count'], keep='first', inplace=True)
    merged_df.reset_index(drop=True, inplace=True)

    print(f'Total reviews: {merged_df.shape[0]}')
    merged_df.to_excel(output_file, index=False)

    return merged_df


def main() -> pd.DataFrame:
    """
    Collects reviews from various websites and puts them into one pd.DataFrame.

    Returns:
        pd.DataFrame with all reviews
    """
    # search_years = {2023, 2024, 2025}

    # формирование выгрузки
    final_filename = 'res/_devices_2023_2025.xlsx'
    tables = [f'res/{file}' for file in os.listdir('res')
              if file.endswith('xlsx') and file != final_filename]
    final_df = form_final_dataframe(tables, final_filename)
    return final_df


if __name__ == '__main__':
    # настройка драйвера
    # driver = webdriver.Chrome(options=configure_chrome_options())
    driver = Driver(uc=True, headless=False)
    ua = UserAgent()
    headers = {'User-Agent': ua.chrome,
               'accept': 'application/json, text/javascript, */*; q=0.01',
               'content-type': 'application/x-www-form-urlencoded; ' +
                               'charset=UTF-8'}
    # настройки вк
    ACCESS_TOKEN = ('f10f3c8af10f3c8af10f3c8a6bf21c4aaaff10ff10f3c8a9520d2' +
                    '9c543503777d2aa814')
    api = vk.API(access_token=ACCESS_TOKEN)
    # создаем папку для файлов с отзывами
    if not os.path.exists('res'):
        os.makedirs('res')
    reviews_df = main()
    driver.quit()
